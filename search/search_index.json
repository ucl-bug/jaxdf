{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"<code>jaxdf</code>","text":"<p><code>jaxdf</code> is a customizable framework for writing differentiable simulators, that decouples the mathematical definition of the problem from the underlying discretization.</p> <p>The underlying computations are performed using JAX, and are thus compatible with the broad set of program transformations, such as automatic differentiation and automatic batching. This enables rapid prototyping of multiple customized representations for a given problem, to develop physics-based neural network layers and to write custom physics losses, while maintaining the speed and flexibility required for research applications.</p> <p>It also contains a growing open-source library of differentiable discretizations compatible with the JAX ecosystem.</p> <p></p>"},{"location":"index.html#tutorials","title":"Tutorials","text":"<ul> <li>Quickstart</li> <li>Physics informed neural nerworks</li> <li>Optimize acoustic simulations</li> <li>(\u26a0\ufe0f WIP) Discretization API</li> <li>Solving the Helmholtz equation with PINNs</li> </ul>"},{"location":"changelog.html","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog.html#unreleased","title":"Unreleased","text":""},{"location":"changelog.html#028-2024-09-17","title":"0.2.8 - 2024-09-17","text":""},{"location":"changelog.html#fixed","title":"Fixed","text":"<ul> <li>Fixed <code>util.get_implemented</code> bug that was happening with the new version of <code>plum</code></li> </ul>"},{"location":"changelog.html#removed","title":"Removed","text":"<ul> <li>Removed the deprecated <code>util._get_implemented</code> function</li> </ul>"},{"location":"changelog.html#027-2023-11-24","title":"0.2.7 - 2023-11-24","text":""},{"location":"changelog.html#changed","title":"Changed","text":"<ul> <li>The Quickstart tutorial has been upgdated.</li> <li>The property <code>Field.ndim</code> has now been moved into <code>Field.domain.ndim</code>, as it is fundamentally a property of the domain</li> <li>The <code>init_params</code> function now will inherit the default parameters from its operator, to remove any source of ambiguity. This means that it should not have any default values, and an error is raised if it does.</li> </ul>"},{"location":"changelog.html#removed_1","title":"Removed","text":"<ul> <li>The <code>__about__</code> file has been removed, as it is redundant</li> <li>The function <code>params_map</code> is removed, use <code>jax.tree_util.tree_map</code> instead.</li> <li>Operators are now expected to return only their outputs, and not parameters. If you need to get the parameters of an operator use its <code>default_params</code> method. To minimize problems for packages relying on <code>jaxdf</code>, in this release the outputs of an <code>operator</code> are filtered to keep only the first one. This will change soon to allow the user to return arbitrary PyTrees.</li> </ul>"},{"location":"changelog.html#added","title":"Added","text":"<ul> <li>JaxDF <code>Field</code>s are now based on equinox. In theory, this should allow to use <code>jaxdf</code> with all the scientific libraries for the jax ecosystem. In practice, please raise an issue when you encounter one of the inevitable bugs :)</li> <li>The new <code>operator.abstract</code> decorator can be used to define an unimplemented operator, for specifying input arguments and docstrings.</li> <li><code>Linear</code> fields are now defined as equal if they have the same set of parameters and the same <code>Domain</code>.</li> <li><code>Ongrid</code> fields now have the method <code>.add_dim()</code>, which adds an extra tailing dimension to its parameters. This is not an in-place update: the method returns a new field.</li> <li>The function <code>jaxdf.util.get_implemented</code> is now exposed to the user.</li> <li>Added <code>laplacian</code> operator for <code>FiniteDifferences</code> fields.</li> <li>JaxDF now uses standard Python logging. To set the logging level, use <code>jaxdf.logger.set_logging_level</code>, for example <code>jaxdf.logger.set_logging_level(\"DEBUG\")</code>. The default level is <code>INFO</code>.</li> <li>Fields have now a handy property <code>.\u03b8</code> which is an alias for <code>.params</code></li> <li><code>Continuous</code> and <code>Linear</code> fields now have the <code>.is_complex</code> property</li> <li><code>Field</code> and <code>Domain</code> are now <code>Modules</code>s, which are based on from <code>equinox.Module</code>. They are entirely equivalent to <code>equinox.Module</code>, but have the extra <code>.replace</code> method that is used to update a single field.</li> </ul>"},{"location":"changelog.html#deprecated","title":"Deprecated","text":"<ul> <li>The property <code>.is_field_complex</code> is now deprecated in favor of <code>.is_complex</code>. Same goes for <code>.is_real</code>.</li> <li><code>Field.get_field</code> is now deprecated in favor of the <code>__call__</code> method.</li> <li>The <code>@discretization</code> decorator is deprecated, as now <code>Fields</code> are <code>equinox</code> modules. It is just not needed now, and until removed it will act as a simple pass-trough</li> </ul>"},{"location":"changelog.html#fixed_1","title":"Fixed","text":"<ul> <li><code>OnGrid.from_grid</code> now automatically adds a dimension at the end of the array for scalar fields, if needed</li> <li>Added a custom operator for <code>equinox.internal._omega._Meta\u03c9</code> objects and Fields, which makes the library compatible with <code>diffrax</code></li> </ul>"},{"location":"changelog.html#026-2023-06-28","title":"0.2.6 - 2023-06-28","text":""},{"location":"changelog.html#changed_1","title":"Changed","text":"<ul> <li>removed <code>jaxlib</code> from dependencies. See https://github.com/google/jax/discussions/16380 for more information</li> </ul>"},{"location":"changelog.html#025-2023-06-23","title":"0.2.5 - 2023-06-23","text":""},{"location":"changelog.html#fixed_2","title":"Fixed","text":"<ul> <li>The default_parameters function now works with custom field types</li> <li><code>__rpow__</code> for <code>OnGrid</code></li> <li>Avoids changing parameters of <code>OnGrid</code> inside jax transformations</li> <li>Spectral gradient for signal of even length now treats the Nyquist frequency correctly</li> <li>Staggering in <code>FiniteDifferences</code> kernel</li> <li>Incorrect behaviour for 3d staggered derivatives</li> </ul>"},{"location":"changelog.html#added_1","title":"Added","text":"<ul> <li>Heterogeneous laplacian operator</li> <li><code>FourierSeries</code> values on arbitrary point using <code>__call__</code> method</li> <li>Automatically infer missing dimension for scalar fields</li> <li>Shift operator</li> <li>Staggering for <code>FourierSeries</code> differential operators</li> </ul>"},{"location":"changelog.html#changed_2","title":"Changed","text":"<ul> <li>Updated docs</li> <li>Renamed ode variable update, removed wrong test in utils</li> <li>Updated support/packaging files</li> </ul>"},{"location":"contributing.html","title":"Contributing","text":"<p>Contributions in the form of pull requests are very welcome! Here's how to get started.</p> <p></p>"},{"location":"contributing.html#getting-started","title":"Getting started","text":"<p>First, fork the library on GitHub. You can do this by clicking on the <code>Fork</code> button in the GitHub interface.</p> <p>Next, clone and install the library in development mode:</p> <pre><code>git clone git@github.com:YOUR_GIT_USERNAME/jaxdf.git\ncd jaxdf\npip install poetry\npoetry install\n</code></pre> <p>After that, install the pre-commit hooks:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p></p>"},{"location":"contributing.html#if-youre-making-changes-to-the-code","title":"If you're making changes to the code","text":"<p>Run <code>git checkout -b my_contribution</code> and make your changes. Be sure to include additional tests if necessary. Increasing the coverage in the coverage report would be great! \ud83d\ude03</p> <p>After making your changes, verify that all tests pass.</p> <pre><code>coverage run --source=jaxdf -m pytest -xvs\n</code></pre> <p>Once you are satisfied with your changes, add an entry to the changelog using kacl-cli, for example:</p> <pre><code>kacl-cli add fixed \"Fixed the unfixable issue \ud83c\udf89\" --modify\n</code></pre> <p>For more information on the types of changes that can be added to the changelog, visit this page.</p> <p>Then commit and push your changes back to your fork of the repository:</p> <pre><code>git push\n</code></pre> <p>Finally, open a pull request on GitHub! You can do this by clicking on the Pull Request button in the GitHub interface.</p> <p>Wait for the CI to run, and one of the developers will review your PR.</p> <p></p>"},{"location":"contributing.html#if-youre-making-changes-to-the-documentation","title":"If you're making changes to the documentation","text":"<p>Make your changes, and then build the documentation using:</p> <pre><code>mkdocs serve\n</code></pre> <p>Please note that due to the way <code>operator</code>s are documented, this might take some time.</p> <p>You can view your local copy of the documentation by navigating to <code>localhost:8000</code> in a web browser.</p>"},{"location":"conv.html","title":"<code>jaxdf.conv</code>","text":"<p>The <code>conv</code> module contains some functions behind the numerical implementation of Finite Differences operators and related functionality.</p>"},{"location":"conv.html#jaxdf.conv.bubble_sort_abs_value","title":"<code>bubble_sort_abs_value(points_list)</code>","text":"<p>Sorts a sequence of grid points by their absolute value.</p> <p>Sorting is done in place. This function is written with numpy, so it can't be transformed by JAX.</p> <p>Example</p> <pre><code>lst = [-3, -2, -1, 0, 1, 2, 3]\nsorted_lst = bubble_sort_abs_value(lst)\nprint(sorted_lst)\n# [0, 1, -1, 2, -2, 3, -3]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>points_list</code> <code>List[Union[float, int]]</code> <p>The grid points to sort.</p> required <p>Returns:</p> Type Description <code>List[Union[float, int]]</code> <p>The sorted grid points.</p> Source code in <code>jaxdf/conv.py</code> <pre><code>def bubble_sort_abs_value(\n    points_list: List[Union[float, int]]) -&gt; List[Union[float, int]]:\n  r\"\"\"Sorts a sequence of grid points by their absolute value.\n\n    Sorting is done __in place__. This function is written with numpy, so it can't\n    be transformed by JAX.\n\n    !!! example\n        ```python\n        lst = [-3, -2, -1, 0, 1, 2, 3]\n        sorted_lst = bubble_sort_abs_value(lst)\n        print(sorted_lst)\n        # [0, 1, -1, 2, -2, 3, -3]\n        ```\n\n    Args:\n      points_list (List[Union[float, int]]): The grid points to sort.\n\n    Returns:\n      The sorted grid points.\n    \"\"\"\n\n  for i in range(len(points_list)):\n    for j in range(0, len(points_list) - i - 1):\n      magnitude_condition = abs(points_list[j]) &gt; abs(points_list[j + 1])\n      same_mag_condition = abs(points_list[j]) == abs(points_list[j + 1])\n      sign_condition = np.sign(points_list[j]) &lt; np.sign(points_list[j + 1])\n      if magnitude_condition or (same_mag_condition and sign_condition):\n        temp = points_list[j]\n        points_list[j] = points_list[j + 1]\n        points_list[j + 1] = temp\n\n  return points_list\n</code></pre>"},{"location":"conv.html#jaxdf.conv.fd_coefficients_fornberg","title":"<code>fd_coefficients_fornberg(order, grid_points, x0)</code>","text":"<p>Generate finite difference stencils for a given order and grid points, using the Fornberg algorithm described in [Fornberg, 2018].</p> <p>The grid points can be placed in any order, can be at arbitrary locations (for example, to implemente staggered stencils) and don't need to be equidistant. The stencil is evaluated for a point in <code>x0</code>. Note that setting <code>order=0</code> will generate interpolation coefficients for the point <code>x0</code>.</p> <p>Example</p> <pre><code>grid_points = [0, 1, 2, -1, -2]\nx0 = 0.0\norder = 2\nstencil, nodes = fd_coefficients_fornberg(order, grid_points, x0)\nprint(f\"Stencil: {stencil}, Nodes: {nodes}\")\n# Stencil: [-0.08333333  1.33333333 -2.5         1.33333333 -0.08333333], Nodes: [-2 -1  0  1  2]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>The order of the stencil.</p> required <code>grid_points</code> <code>List[Union[float, int]]</code> <p>The grid points to use.</p> required <code>x0</code> <code>Union[float, int]</code> <p>The point at which to evaluate the stencil.</p> required <p>Returns:</p> Type Description <code>Tuple[List[None], List[Union[float, int]]]</code> <p>The stencil and the grid points where the stencil is evaluated.</p> Source code in <code>jaxdf/conv.py</code> <pre><code>@no_type_check\ndef fd_coefficients_fornberg(\n    order: int, grid_points: List[Union[float, int]],\n    x0: Union[float, int]) -&gt; Tuple[List[None], List[Union[float, int]]]:\n  r\"\"\"Generate finite difference stencils for a given order and grid points, using\n    the Fornberg algorithm described in [[Fornberg, 2018]](https://web.njit.edu/~jiang/math712/fornberg.pdf).\n\n    The grid points can be placed in any order, can be at arbitrary locations (for example, to implemente staggered\n    stencils) and don't need to be equidistant.\n    The stencil is evaluated for a point in `x0`. Note that setting `order=0` will generate interpolation coefficients\n    for the point `x0`.\n\n    !!! example\n        ```python\n        grid_points = [0, 1, 2, -1, -2]\n        x0 = 0.0\n        order = 2\n        stencil, nodes = fd_coefficients_fornberg(order, grid_points, x0)\n        print(f\"Stencil: {stencil}, Nodes: {nodes}\")\n        # Stencil: [-0.08333333  1.33333333 -2.5         1.33333333 -0.08333333], Nodes: [-2 -1  0  1  2]\n        ```\n\n    Args:\n      order (int): The order of the stencil.\n      grid_points (List[Union[float, int]]): The grid points to use.\n      x0 (Union[float, int]): The point at which to evaluate the stencil.\n\n    Returns:\n      The stencil and the grid points where the stencil is evaluated.\n    \"\"\"\n  # from Generation of Finite Difference Formulas on Arbitrarily Spaced Grids\n  # Bengt Fornberg, 1998\n  # https://web.njit.edu/~jiang/math712/fornberg.pdf\n  M = order\n  N = len(grid_points) - 1\n\n  # Sort the grid points\n  alpha = bubble_sort_abs_value(grid_points)\n  delta = dict()    # key: (m,n,v)\n  delta[(0, 0, 0)] = 1.0\n  c1 = 1.0\n\n  for n in range(1, N + 1):\n    c2 = 1.0\n    for v in range(n):\n      c3 = alpha[n] - alpha[v]\n      c2 = c2 * c3\n      if n &lt; M:\n        delta[(n, n - 1, v)] = 0.0\n      for m in range(min([n, M]) + 1):\n        d1 = delta[(m, n - 1, v)] if (m, n - 1, v) in delta.keys() else 0.0\n        d2 = (delta[(m - 1, n - 1, v)] if\n              (m - 1, n - 1, v) in delta.keys() else 0.0)\n        delta[(m, n, v)] = ((alpha[n] - x0) * d1 - m * d2) / c3\n\n    for m in range(min([n, M]) + 1):\n      d1 = (delta[(m - 1, n - 1, n - 1)] if\n            (m - 1, n - 1, n - 1) in delta.keys() else 0.0)\n      d2 = delta[(m, n - 1, n - 1)] if (m, n - 1,\n                                        n - 1) in delta.keys() else 0.0\n      delta[(m, n, n)] = (c1 / c2) * (m * d1 - (alpha[n - 1] - x0) * d2)\n    c1 = c2\n\n  # Extract the delta with m = M and n = N\n  coeffs = [None] * (N + 1)\n  for key in delta:\n    if key[0] == M and key[1] == N:\n      coeffs[key[2]] = delta[key]\n\n  # sort coefficeient and alpha by alpha\n  idx = np.argsort(alpha)\n  alpha = np.take_along_axis(np.asarray(alpha), idx, axis=-1)\n  coeffs = np.take_along_axis(np.asarray(coeffs), idx, axis=-1)\n\n  return coeffs, alpha\n</code></pre>"},{"location":"conv.html#jaxdf.conv.reflection_conv","title":"<code>reflection_conv(kernel, array, reverse=True)</code>","text":"<p>Convolves an array with a kernel, using reflection padding. The kernel is supposed to have the same number of dimensions as the array.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>ndarray</code> <p>The kernel to convolve with.</p> required <code>array</code> <code>ndarray</code> <p>The array to convolve.</p> required <code>reverse</code> <code>bool</code> <p>Whether to reverse the kernel before convolving. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The convolved array.</p> Source code in <code>jaxdf/conv.py</code> <pre><code>def reflection_conv(kernel: jnp.ndarray,\n                    array: jnp.ndarray,\n                    reverse: bool = True) -&gt; jnp.ndarray:\n  r\"\"\"Convolves an array with a kernel, using reflection padding.\n    The kernel is supposed to have the same number of dimensions as the array.\n\n    Args:\n      kernel (jnp.ndarray): The kernel to convolve with.\n      array (jnp.ndarray): The array to convolve.\n      reverse (bool, optional): Whether to reverse the kernel before convolving.\n        Defaults to True.\n\n    Returns:\n      The convolved array.\n    \"\"\"\n  # Reflection padding the array appropriately\n  pad = [((x - 1) // 2, (x - 1) // 2) for x in kernel.shape]\n  f = jnp.pad(array, pad, mode=\"wrap\")\n\n  if reverse:\n    # Reverse the kernel over all axes\n    kernel = jnp.flip(kernel, axis=tuple(range(kernel.ndim)))\n\n  # Apply kernel\n  return jsp.signal.convolve(f, kernel, mode=\"valid\")\n</code></pre>"},{"location":"core.html","title":"<code>jaxdf.core</code>","text":""},{"location":"core.html#module-overview","title":"Module Overview","text":"<p>This module is the fundamental part of the <code>jaxdf</code> framework.</p> <p>At its core is the <code>Field</code> class, a key element of <code>jaxdf</code>. This class is designed as a module derived from <code>equinox.Module</code>, which means it's a JAX-compatible dataclass. All types of discretizations within <code>jaxdf</code> are derived from the <code>Field</code> class.</p> <p>Another crucial feature of <code>jaxdf</code> is the <code>operator</code> decorator. This decorator enables the implementation of multiple-dispatch functionality through the <code>plum</code> library. This is particularly useful for creating new operators within the framework.</p>"},{"location":"core.html#jaxdf.core.operator","title":"<code>operator = Operator()</code>  <code>module-attribute</code>","text":"<p>Decorator for defining operators using multiple dispatch. The type annotation of the <code>evaluate</code> function are used to determine the dispatch rules. The dispatch syntax is the same as the Julia one, that is: operators are dispatched on the types of the positional arguments.</p> <p>Parameters:</p> Name Type Description Default <code>evaluate</code> <code>Callable</code> <p>A function with the signature <code>evaluate(field, *args, **kwargs, params)</code>.   It must return a tuple, with the first element being a field and the second   element being the default parameters for the operator.</p> required <code>init_params</code> <code>Callable</code> <p>A function that overrides the default parameters initializer for the   operator. Useful when running the operator just to get the parameters is expensive.</p> required <code>precedence</code> <code>int</code> <p>The precedence of the operator if an ambiguous match is found.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <p>The operator function with signature <code>evaluate(field, *args, **kwargs, params)</code>.</p> <p>Keyword arguments are not considered for dispatching. Keyword arguments are defined after the <code>*</code> in the function signature.</p> <p>Example</p> <pre><code>@operator\ndef my_operator(x: FourierSeries, *, dx: float, params=None):\n  ...\n</code></pre> <p>The argument <code>params</code> is mandatory and it must be a keyword argument. It is used to pass the parameters of the operator, for example the stencil coefficients of a finite difference operator.</p> <p>The default value of the parameters is specified by the <code>init_params</code> function, as follows:</p> <p>Example</p> <pre><code>def params_initializer(x, *, dx):\n  return {\"stencil\": jnp.ones(x.shape) * dx}\n\n@operator(init_params=params_initializer)\ndef my_operator(x, *, dx, params=None):\n  b = params[\"stencil\"] / dx\n  y_params = jnp.convolve(x.params, b, mode=\"same\")\n  return x.replace_params(y_params)\n</code></pre> <p>The default value of <code>params</code> is not considered during computation. If the operator has no parameters, the <code>init_params</code> function can be omitted. In this case, the <code>params</code> value is set to <code>None</code>.</p> <p>For constant parameters, the <code>constants</code> function can be used:</p> <p>Example</p> <pre><code>@operator(init_params=constants({\"a\": 1, \"b\": 2.0}))\n  def my_operator(x, *, params):\n  return x + params[\"a\"] + params[\"b\"]\n</code></pre>"},{"location":"core.html#jaxdf.core.Field","title":"<code>Field</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>jaxdf/core.py</code> <pre><code>class Field(Module):\n  params: PyTree\n  domain: Domain\n\n  # For concise code\n  @property\n  def \u03b8(self):\n    r\"\"\"Handy alias for the `params` attribute\"\"\"\n    return self.params\n\n  def __call__(self, x):\n    r\"\"\"\n        An Field can be called as a function, returning the field at a\n        desired point.\n\n        !!! example\n            ```python\n            ...\n            a = Continuous.from_function(init_params, domain, get_field)\n            field_at_x = a(1.0)\n            ```\n        \"\"\"\n    raise NotImplementedError(\n        f\"Not implemented for {self.__class__.__name__} discretization\")\n\n  @property\n  def on_grid(self):\n    \"\"\"Returns the field on the grid points of the domain.\"\"\"\n    raise NotImplementedError(\n        f\"Not implemented for {self.__class__.__name__} discretization\")\n\n  @property\n  def dims(self):\n    r\"\"\"The dimension of the field values\"\"\"\n    raise NotImplementedError\n\n  @property\n  def is_complex(self) -&gt; bool:\n    r\"\"\"Checks if a field is complex.\n\n        Returns:\n          bool: Whether the field is complex.\n        \"\"\"\n    raise NotImplementedError\n\n  @property\n  def is_field_complex(self) -&gt; bool:\n    warnings.warn(\n        \"Field.is_field_complex is deprecated. Use Field.is_complex instead.\",\n        DeprecationWarning,\n    )\n    return self.is_complex\n\n  @property\n  def is_real(self) -&gt; bool:\n    return not self.is_complex\n\n  def replace_params(self, new_params):\n    r\"\"\"Returns a new field of the same type, with the same domain and auxiliary data, but with new parameters.\n\n        !!! example\n            ```python\n            x = FourierSeries(jnp.ones(10), domain=domain)\n            y_params = x.params + 1\n            y = x.replace_params(y_params)\n            ```\n\n        Args:\n          new_params (Any): The new parameters.\n\n        Returns:\n          Field: A new field with the same domain and auxiliary data, but with new parameters.\n        \"\"\"\n    return self.__class__(new_params, self.domain)\n\n  # Dummy magic functions to make it work with\n  # the dispatch system\n  def __add__(self, other):\n    return __add__(self, other)\n\n  def __radd__(self, other):\n    return __radd__(self, other)\n\n  def __sub__(self, other):\n    return __sub__(self, other)\n\n  def __rsub__(self, other):\n    return __rsub__(self, other)\n\n  def __mul__(self, other):\n    return __mul__(self, other)\n\n  def __rmul__(self, other):\n    return __rmul__(self, other)\n\n  def __neg__(self):\n    return __neg__(self)\n\n  def __pow__(self, other):\n    return __pow__(self, other)\n\n  def __rpow__(self, other):\n    return __rpow__(self, other)\n\n  def __truediv__(self, other):\n    return __truediv__(self, other)\n\n  def __rtruediv__(self, other):\n    return __rtruediv__(self, other)\n</code></pre>"},{"location":"core.html#jaxdf.core.Field.dims","title":"<code>dims</code>  <code>property</code>","text":"<p>The dimension of the field values</p>"},{"location":"core.html#jaxdf.core.Field.is_complex","title":"<code>is_complex: bool</code>  <code>property</code>","text":"<p>Checks if a field is complex.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the field is complex.</p>"},{"location":"core.html#jaxdf.core.Field.on_grid","title":"<code>on_grid</code>  <code>property</code>","text":"<p>Returns the field on the grid points of the domain.</p>"},{"location":"core.html#jaxdf.core.Field.\u03b8","title":"<code>\u03b8</code>  <code>property</code>","text":"<p>Handy alias for the <code>params</code> attribute</p>"},{"location":"core.html#jaxdf.core.Field.__call__","title":"<code>__call__(x)</code>","text":"<p>An Field can be called as a function, returning the field at a desired point.</p> <p>Example</p> <pre><code>...\na = Continuous.from_function(init_params, domain, get_field)\nfield_at_x = a(1.0)\n</code></pre> Source code in <code>jaxdf/core.py</code> <pre><code>def __call__(self, x):\n  r\"\"\"\n      An Field can be called as a function, returning the field at a\n      desired point.\n\n      !!! example\n          ```python\n          ...\n          a = Continuous.from_function(init_params, domain, get_field)\n          field_at_x = a(1.0)\n          ```\n      \"\"\"\n  raise NotImplementedError(\n      f\"Not implemented for {self.__class__.__name__} discretization\")\n</code></pre>"},{"location":"core.html#jaxdf.core.Field.replace_params","title":"<code>replace_params(new_params)</code>","text":"<p>Returns a new field of the same type, with the same domain and auxiliary data, but with new parameters.</p> <p>Example</p> <pre><code>x = FourierSeries(jnp.ones(10), domain=domain)\ny_params = x.params + 1\ny = x.replace_params(y_params)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>Any</code> <p>The new parameters.</p> required <p>Returns:</p> Name Type Description <code>Field</code> <p>A new field with the same domain and auxiliary data, but with new parameters.</p> Source code in <code>jaxdf/core.py</code> <pre><code>def replace_params(self, new_params):\n  r\"\"\"Returns a new field of the same type, with the same domain and auxiliary data, but with new parameters.\n\n      !!! example\n          ```python\n          x = FourierSeries(jnp.ones(10), domain=domain)\n          y_params = x.params + 1\n          y = x.replace_params(y_params)\n          ```\n\n      Args:\n        new_params (Any): The new parameters.\n\n      Returns:\n        Field: A new field with the same domain and auxiliary data, but with new parameters.\n      \"\"\"\n  return self.__class__(new_params, self.domain)\n</code></pre>"},{"location":"core.html#jaxdf.core.Operator","title":"<code>Operator</code>","text":"Source code in <code>jaxdf/core.py</code> <pre><code>class Operator:\n\n  def __call__(\n      self,\n      evaluate: Union[Callable, None] = None,\n      init_params: Union[Callable, None] = None,\n      precedence: int = 0,\n  ):\n    if evaluate is None:\n      # Returns the decorator\n      def decorator(evaluate):\n        return _operator(evaluate, precedence, init_params)\n\n      return decorator\n    else:\n      return _operator(evaluate, precedence, init_params)\n\n  def abstract(self, evaluate: Callable):\n    \"\"\"Decorator for defining abstract operators. This is mainly used\n        to define generic docstrings.\"\"\"\n    return _abstract_operator(evaluate)\n</code></pre>"},{"location":"core.html#jaxdf.core.Operator.abstract","title":"<code>abstract(evaluate)</code>","text":"<p>Decorator for defining abstract operators. This is mainly used to define generic docstrings.</p> Source code in <code>jaxdf/core.py</code> <pre><code>def abstract(self, evaluate: Callable):\n  \"\"\"Decorator for defining abstract operators. This is mainly used\n      to define generic docstrings.\"\"\"\n  return _abstract_operator(evaluate)\n</code></pre>"},{"location":"core.html#jaxdf.core.constants","title":"<code>constants(value)</code>","text":"<p>This is a higher order function for defining constant parameters of operators, independent of the operator arguments.</p> <p>Example</p> <pre><code>@operator(init_params=constants({\"a\": 1, \"b\": 2.0}))\ndef my_operator(x, *, params):\n  return x + params[\"a\"] + params[\"b\"]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value of the constant parameters.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The parameters initializer function that returns the constant value.</p> Source code in <code>jaxdf/core.py</code> <pre><code>def constants(value) -&gt; Callable:\n  r\"\"\"This is a higher order function for defining constant parameters of\n    operators, independent of the operator arguments.\n\n    !!! example\n\n        ```python\n        @operator(init_params=constants({\"a\": 1, \"b\": 2.0}))\n        def my_operator(x, *, params):\n          return x + params[\"a\"] + params[\"b\"]\n        ```\n\n    Args:\n      value (Any): The value of the constant parameters.\n\n    Returns:\n      Callable: The parameters initializer function that returns the constant value.\n    \"\"\"\n\n  def init_params(*args, **kwargs):\n    return value\n\n  return init_params\n</code></pre>"},{"location":"discretization.html","title":"discretization","text":""},{"location":"discretization.html#jaxdf.discretization.Continuous","title":"<code>Continuous</code>","text":"<p>             Bases: <code>Field</code></p> <p>A continuous discretization. This discretization assumes that the field is a function of the parameters contained in <code>Continuous.params</code> and a point in the domain. The function that computes the field from the parameters and the point in the domain is contained in <code>Continuous.get_fun</code>. This function has the signature <code>get_fun(params, x)</code>, where <code>params</code> is the parameter vector and <code>x</code> is the point in the domain.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>class Continuous(Field):\n  r\"\"\"A continuous discretization. This discretization assumes that the field is a\n    function of the parameters contained in `Continuous.params` and a point in the\n    domain. The function that computes the field from the parameters and the point in\n    the domain is contained in `Continuous.get_fun`. This function has the signature\n    `get_fun(params, x)`, where `params` is the parameter vector and `x` is the point\n    in the domain.\"\"\"\n  params: PyTree\n  domain: Domain\n  get_fun: Callable = eqx.field(static=True)\n\n  def __eq__(self, other):\n    return tree_equal(self, other) * (self.get_fun == other.get_fun)\n\n  @property\n  def is_complex(self) -&gt; bool:\n    r\"\"\"Checks if a field is complex.\n\n        Returns:\n          bool: Whether the field is complex.\n        \"\"\"\n    origin = self.domain.origin\n    value = self.get_fun(self.params, origin)\n    return jnp.iscomplexobj(value)\n\n  @property\n  def dims(self):\n    x = self.domain.origin\n    return eval_shape(self.get_fun, self.params, x).shape\n\n  def replace_params(self, new_params: PyTree) -&gt; \"Continuous\":\n    r\"\"\"Replaces the parameters of the discretization with new ones. The domain\n        and `get_field` function are not changed.\n\n        Args:\n          new_params (PyTree): The new parameters of the discretization.\n\n        Returns:\n          Continuous: A continuous discretization with the new parameters.\n        \"\"\"\n    return self.__class__(new_params, self.domain, self.get_field)\n\n  def update_fun_and_params(\n      self,\n      params: PyTree,\n      get_field: Callable,\n  ) -&gt; \"Continuous\":\n    r\"\"\"Updates the parameters and the function of the discretization.\n\n        Args:\n          params (PyTree): The new parameters of the discretization.\n          get_field (Callable): A function that takes a parameter vector and a point in\n            the domain and returns the field at that point. The signature of this\n            function is `get_field(params, x)`.\n\n        Returns:\n          Continuous: A continuous discretization with the new parameters and function.\n        \"\"\"\n    return self.__class__(params, self.domain, get_field)\n\n  @classmethod\n  def from_function(cls, domain, init_fun: Callable, get_field: Callable,\n                    seed):\n    r\"\"\"Creates a continuous discretization from an `init_fun` function.\n\n        Args:\n          domain (Domain): The domain of the discretization.\n          init_fun (Callable): A function that initializes the parameters of the\n            discretization. The signature of this function is `init_fun(rng, domain)`.\n          get_field (Callable): A function that takes a parameter vector and a point in\n            the domain and returns the field at that point. The signature of this\n            function is `get_field(params, x)`.\n          seed (int): The seed for the random number generator.\n\n        Returns:\n          Continuous: A continuous discretization.\n        \"\"\"\n    params = init_fun(seed, domain)\n    return cls(params, domain=domain, get_fun=get_field)\n\n  def __call__(self, x):\n    r\"\"\"\n        An object of this class can be called as a function, returning the field at a\n        desired point.\n\n        !!! example\n            ```python\n            ...\n            a = Continuous.from_function(init_params, domain, get_field)\n            field_at_x = a(1.0)\n            ```\n        \"\"\"\n    return self.get_fun(self.params, x)\n\n  def get_field(self, x):\n    warnings.warn(\n        \"Continuous.get_field is deprecated. Use Continuous.__call__ instead.\",\n        DeprecationWarning,\n    )\n    return self.__call__(x)\n\n  @property\n  def on_grid(self):\n    \"\"\"Returns the field on the grid points of the domain.\"\"\"\n    fun = self.get_fun\n    ndims = len(self.domain.N)\n    for _ in range(ndims):\n      fun = vmap(fun, in_axes=(None, 0))\n\n    return fun(self.params, self.domain.grid)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.Continuous.is_complex","title":"<code>is_complex: bool</code>  <code>property</code>","text":"<p>Checks if a field is complex.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the field is complex.</p>"},{"location":"discretization.html#jaxdf.discretization.Continuous.on_grid","title":"<code>on_grid</code>  <code>property</code>","text":"<p>Returns the field on the grid points of the domain.</p>"},{"location":"discretization.html#jaxdf.discretization.Continuous.__call__","title":"<code>__call__(x)</code>","text":"<p>An object of this class can be called as a function, returning the field at a desired point.</p> <p>Example</p> <pre><code>...\na = Continuous.from_function(init_params, domain, get_field)\nfield_at_x = a(1.0)\n</code></pre> Source code in <code>jaxdf/discretization.py</code> <pre><code>def __call__(self, x):\n  r\"\"\"\n      An object of this class can be called as a function, returning the field at a\n      desired point.\n\n      !!! example\n          ```python\n          ...\n          a = Continuous.from_function(init_params, domain, get_field)\n          field_at_x = a(1.0)\n          ```\n      \"\"\"\n  return self.get_fun(self.params, x)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.Continuous.from_function","title":"<code>from_function(domain, init_fun, get_field, seed)</code>  <code>classmethod</code>","text":"<p>Creates a continuous discretization from an <code>init_fun</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>The domain of the discretization.</p> required <code>init_fun</code> <code>Callable</code> <p>A function that initializes the parameters of the discretization. The signature of this function is <code>init_fun(rng, domain)</code>.</p> required <code>get_field</code> <code>Callable</code> <p>A function that takes a parameter vector and a point in the domain and returns the field at that point. The signature of this function is <code>get_field(params, x)</code>.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> required <p>Returns:</p> Name Type Description <code>Continuous</code> <p>A continuous discretization.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>@classmethod\ndef from_function(cls, domain, init_fun: Callable, get_field: Callable,\n                  seed):\n  r\"\"\"Creates a continuous discretization from an `init_fun` function.\n\n      Args:\n        domain (Domain): The domain of the discretization.\n        init_fun (Callable): A function that initializes the parameters of the\n          discretization. The signature of this function is `init_fun(rng, domain)`.\n        get_field (Callable): A function that takes a parameter vector and a point in\n          the domain and returns the field at that point. The signature of this\n          function is `get_field(params, x)`.\n        seed (int): The seed for the random number generator.\n\n      Returns:\n        Continuous: A continuous discretization.\n      \"\"\"\n  params = init_fun(seed, domain)\n  return cls(params, domain=domain, get_fun=get_field)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.Continuous.replace_params","title":"<code>replace_params(new_params)</code>","text":"<p>Replaces the parameters of the discretization with new ones. The domain and <code>get_field</code> function are not changed.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>PyTree</code> <p>The new parameters of the discretization.</p> required <p>Returns:</p> Name Type Description <code>Continuous</code> <code>Continuous</code> <p>A continuous discretization with the new parameters.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>def replace_params(self, new_params: PyTree) -&gt; \"Continuous\":\n  r\"\"\"Replaces the parameters of the discretization with new ones. The domain\n      and `get_field` function are not changed.\n\n      Args:\n        new_params (PyTree): The new parameters of the discretization.\n\n      Returns:\n        Continuous: A continuous discretization with the new parameters.\n      \"\"\"\n  return self.__class__(new_params, self.domain, self.get_field)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.Continuous.update_fun_and_params","title":"<code>update_fun_and_params(params, get_field)</code>","text":"<p>Updates the parameters and the function of the discretization.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>PyTree</code> <p>The new parameters of the discretization.</p> required <code>get_field</code> <code>Callable</code> <p>A function that takes a parameter vector and a point in the domain and returns the field at that point. The signature of this function is <code>get_field(params, x)</code>.</p> required <p>Returns:</p> Name Type Description <code>Continuous</code> <code>Continuous</code> <p>A continuous discretization with the new parameters and function.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>def update_fun_and_params(\n    self,\n    params: PyTree,\n    get_field: Callable,\n) -&gt; \"Continuous\":\n  r\"\"\"Updates the parameters and the function of the discretization.\n\n      Args:\n        params (PyTree): The new parameters of the discretization.\n        get_field (Callable): A function that takes a parameter vector and a point in\n          the domain and returns the field at that point. The signature of this\n          function is `get_field(params, x)`.\n\n      Returns:\n        Continuous: A continuous discretization with the new parameters and function.\n      \"\"\"\n  return self.__class__(params, self.domain, get_field)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.FiniteDifferences","title":"<code>FiniteDifferences</code>","text":"<p>             Bases: <code>OnGrid</code></p> <p>A Finite Differences field defined on a collocation grid.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>class FiniteDifferences(OnGrid):\n  r\"\"\"A Finite Differences field defined on a collocation grid.\"\"\"\n  params: PyTree\n  domain: Domain\n  accuracy: int = eqx.field(default=8, static=True)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.FourierSeries","title":"<code>FourierSeries</code>","text":"<p>             Bases: <code>OnGrid</code></p> <p>A Fourier series field defined on a collocation grid.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>class FourierSeries(OnGrid):\n  r\"\"\"A Fourier series field defined on a collocation grid.\"\"\"\n\n  def __call__(self, x):\n    r\"\"\"Uses the Fourier shift theorem to compute the value of the field\n        at an arbitrary point. Requires N*2 one dimensional FFTs.\n\n        Args:\n          x (float): The point at which to evaluate the field.\n\n        Returns:\n          float, jnp.ndarray: The value of the field at the point.\n        \"\"\"\n    dx = jnp.asarray(self.domain.dx)\n    domain_size = jnp.asarray(self.domain.N) * dx\n    shift = x - domain_size / 2 + 0.5 * dx\n\n    k_vec = [\n        jnp.exp(-1j * k * delta) for k, delta in zip(self._freq_axis, shift)\n    ]\n    ffts = self._ffts\n\n    new_params = self.params\n\n    def single_shift(axis, u):\n      u = jnp.moveaxis(u, axis, -1)\n      Fx = ffts[0](u, axis=-1)\n      iku = Fx * k_vec[axis]\n      du = ffts[1](iku, axis=-1, n=u.shape[-1])\n      return jnp.moveaxis(du, -1, axis)\n\n    for ax in range(self.domain.ndim):\n      new_params = single_shift(ax, new_params)\n\n    origin = tuple([0] * self.domain.ndim)\n    return new_params[origin]\n\n  @property\n  def _freq_axis(self):\n    r\"\"\"Returns the frequency axis of the grid\"\"\"\n    if self.is_complex:\n\n      def f(N, dx):\n        return jnp.fft.fftfreq(N, dx) * 2 * jnp.pi\n\n    else:\n\n      def f(N, dx):\n        return jnp.fft.rfftfreq(N, dx) * 2 * jnp.pi\n\n    k_axis = [f(n, delta) for n, delta in zip(self.domain.N, self.domain.dx)]\n    return k_axis\n\n  @property\n  def _ffts(self):\n    r\"\"\"Returns the FFT and iFFT functions that are appropriate for the\n        field type (real or complex)\n        \"\"\"\n    if self.is_real:\n      return [jnp.fft.rfft, jnp.fft.irfft]\n    else:\n      return [jnp.fft.fft, jnp.fft.ifft]\n\n  @property\n  def _cut_freq_axis(self):\n    r\"\"\"Same as `FourierSeries._freq_axis`, but last frequency axis is relative to a real FFT.\n        Those frequency axis match with the ones of the rfftn function\n        \"\"\"\n\n    def f(N, dx):\n      return jnp.fft.fftfreq(N, dx) * 2 * jnp.pi\n\n    k_axis = [f(n, delta) for n, delta in zip(self.domain.N, self.domain.dx)]\n    if not self.is_complex:\n      k_axis[-1] = (jnp.fft.rfftfreq(self.domain.N[-1], self.domain.dx[-1]) *\n                    2 * jnp.pi)\n    return k_axis\n\n  @property\n  def _cut_freq_grid(self):\n    return jnp.stack(jnp.meshgrid(*self._cut_freq_axis, indexing=\"ij\"),\n                     axis=-1)\n\n  @property\n  def _freq_grid(self):\n    return jnp.stack(jnp.meshgrid(*self._freq_axis, indexing=\"ij\"), axis=-1)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.FourierSeries.__call__","title":"<code>__call__(x)</code>","text":"<p>Uses the Fourier shift theorem to compute the value of the field at an arbitrary point. Requires N*2 one dimensional FFTs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The point at which to evaluate the field.</p> required <p>Returns:</p> Type Description <p>float, jnp.ndarray: The value of the field at the point.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>def __call__(self, x):\n  r\"\"\"Uses the Fourier shift theorem to compute the value of the field\n      at an arbitrary point. Requires N*2 one dimensional FFTs.\n\n      Args:\n        x (float): The point at which to evaluate the field.\n\n      Returns:\n        float, jnp.ndarray: The value of the field at the point.\n      \"\"\"\n  dx = jnp.asarray(self.domain.dx)\n  domain_size = jnp.asarray(self.domain.N) * dx\n  shift = x - domain_size / 2 + 0.5 * dx\n\n  k_vec = [\n      jnp.exp(-1j * k * delta) for k, delta in zip(self._freq_axis, shift)\n  ]\n  ffts = self._ffts\n\n  new_params = self.params\n\n  def single_shift(axis, u):\n    u = jnp.moveaxis(u, axis, -1)\n    Fx = ffts[0](u, axis=-1)\n    iku = Fx * k_vec[axis]\n    du = ffts[1](iku, axis=-1, n=u.shape[-1])\n    return jnp.moveaxis(du, -1, axis)\n\n  for ax in range(self.domain.ndim):\n    new_params = single_shift(ax, new_params)\n\n  origin = tuple([0] * self.domain.ndim)\n  return new_params[origin]\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.Linear","title":"<code>Linear</code>","text":"<p>             Bases: <code>Field</code></p> <p>This discretization assumes that the field is a linear function of the parameters contained in <code>Linear.params</code>.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>class Linear(Field):\n  r\"\"\"This discretization assumes that the field is a linear function of the\n    parameters contained in `Linear.params`.\n    \"\"\"\n  params: PyTree\n  domain: Domain\n\n  def __eq__(self, other):\n    return tree_equal(self, other) * (self.domain == other.domain)\n\n  @property\n  def is_complex(self) -&gt; bool:\n    r\"\"\"Checks if a field is complex.\n\n        Returns:\n          bool: Whether the field is complex.\n        \"\"\"\n    return self.params.dtype == jnp.complex64 or self.params.dtype == jnp.complex128\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.Linear.is_complex","title":"<code>is_complex: bool</code>  <code>property</code>","text":"<p>Checks if a field is complex.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the field is complex.</p>"},{"location":"discretization.html#jaxdf.discretization.OnGrid","title":"<code>OnGrid</code>","text":"<p>             Bases: <code>Linear</code></p> Source code in <code>jaxdf/discretization.py</code> <pre><code>class OnGrid(Linear):\n\n  def __check_init__(self):\n    # Check if the number of dimensions of the parameters is correct, fix if needed\n    if len(self.params.shape) == len(self.domain.N):\n      # If only the last one is missing, add it\n      if self.params.shape == self.domain.N:\n        object.__setattr__(self, \"params\", jnp.expand_dims(self.params,\n                                                           axis=-1))\n\n    if self.params.shape == self.domain.N:\n      raise ValueError(\n          f\"The number of dimensions of the parameters is incorrect. It should be the number of dimensions of the domain plus at least one more. The parameters have shape {self.params.shape} and the domain has shape {self.domain.N}\"\n      )\n\n  def add_dim(self):\n    \"\"\"Adds a dimension at the end of the `params` array.\"\"\"\n    new_params = jnp.expand_dims(self.params, axis=-1)\n    # Returns a new object\n    return self.__class__(new_params, self.domain)\n\n  @property\n  def dims(self):\n    return self.params.shape[-1]\n\n  def __getitem__(self, idx):\n    r\"\"\"Allow indexing when leading batch / time dimensions are\n        present in the parameters\n\n        !!! example\n            ```python\n            ...\n            domain = Domain((16, (1.0,))\n\n            # 10 fields\n            params = random.uniform(key, (10, 16, 1))\n            a = OnGrid(params, domain)\n\n            # Field at the 5th index\n            field = a[5]\n            ```\n\n        Returns:\n          OnGrid: A linear discretization on the grid points of the domain.\n\n        Raises:\n          IndexError: If the field is not indexable (single field).\n        \"\"\"\n    if self.domain.ndim + 1 == len(self.params.shape):\n      raise IndexError(\n          \"Indexing is only supported if there's at least one batch / time dimension\"\n      )\n\n    new_params = self.params[idx]\n    return self.__class__(new_params, self.domain)\n\n  @classmethod\n  def empty(cls, domain, dims=1):\n    r\"\"\"Creates an empty OnGrid field (zero field). Equivalent to\n        `OnGrid(jnp.zeros(domain.N), domain)`.\n        \"\"\"\n    _N = list(domain.N) + [dims]\n    N = tuple(_N)\n    return cls(jnp.zeros(N), domain)\n\n  @classmethod\n  def from_grid(cls, grid_values, domain):\n    r\"\"\"Creates an OnGrid field from a grid of values.\n\n        Args:\n          grid_values (ndarray): The grid of values.\n          domain (Domain): The domain of the discretization.\n        \"\"\"\n    a = cls(grid_values, domain)\n    return a\n\n  def replace_params(self, new_params):\n    r\"\"\"Replaces the parameters of the discretization with new ones. The domain\n        is not changed.\n\n        Args:\n          new_params (PyTree): The new parameters of the discretization.\n\n        Returns:\n          OnGrid: A linear discretization with the new parameters.\n        \"\"\"\n    return self.__class__(new_params, self.domain)\n\n  @property\n  def on_grid(self):\n    r\"\"\"The field on the grid points of the domain.\"\"\"\n    return self.params\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.OnGrid.on_grid","title":"<code>on_grid</code>  <code>property</code>","text":"<p>The field on the grid points of the domain.</p>"},{"location":"discretization.html#jaxdf.discretization.OnGrid.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Allow indexing when leading batch / time dimensions are present in the parameters</p> <p>Example</p> <pre><code>...\ndomain = Domain((16, (1.0,))\n\n# 10 fields\nparams = random.uniform(key, (10, 16, 1))\na = OnGrid(params, domain)\n\n# Field at the 5th index\nfield = a[5]\n</code></pre> <p>Returns:</p> Name Type Description <code>OnGrid</code> <p>A linear discretization on the grid points of the domain.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If the field is not indexable (single field).</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>def __getitem__(self, idx):\n  r\"\"\"Allow indexing when leading batch / time dimensions are\n      present in the parameters\n\n      !!! example\n          ```python\n          ...\n          domain = Domain((16, (1.0,))\n\n          # 10 fields\n          params = random.uniform(key, (10, 16, 1))\n          a = OnGrid(params, domain)\n\n          # Field at the 5th index\n          field = a[5]\n          ```\n\n      Returns:\n        OnGrid: A linear discretization on the grid points of the domain.\n\n      Raises:\n        IndexError: If the field is not indexable (single field).\n      \"\"\"\n  if self.domain.ndim + 1 == len(self.params.shape):\n    raise IndexError(\n        \"Indexing is only supported if there's at least one batch / time dimension\"\n    )\n\n  new_params = self.params[idx]\n  return self.__class__(new_params, self.domain)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.OnGrid.add_dim","title":"<code>add_dim()</code>","text":"<p>Adds a dimension at the end of the <code>params</code> array.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>def add_dim(self):\n  \"\"\"Adds a dimension at the end of the `params` array.\"\"\"\n  new_params = jnp.expand_dims(self.params, axis=-1)\n  # Returns a new object\n  return self.__class__(new_params, self.domain)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.OnGrid.empty","title":"<code>empty(domain, dims=1)</code>  <code>classmethod</code>","text":"<p>Creates an empty OnGrid field (zero field). Equivalent to <code>OnGrid(jnp.zeros(domain.N), domain)</code>.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>@classmethod\ndef empty(cls, domain, dims=1):\n  r\"\"\"Creates an empty OnGrid field (zero field). Equivalent to\n      `OnGrid(jnp.zeros(domain.N), domain)`.\n      \"\"\"\n  _N = list(domain.N) + [dims]\n  N = tuple(_N)\n  return cls(jnp.zeros(N), domain)\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.OnGrid.from_grid","title":"<code>from_grid(grid_values, domain)</code>  <code>classmethod</code>","text":"<p>Creates an OnGrid field from a grid of values.</p> <p>Parameters:</p> Name Type Description Default <code>grid_values</code> <code>ndarray</code> <p>The grid of values.</p> required <code>domain</code> <code>Domain</code> <p>The domain of the discretization.</p> required Source code in <code>jaxdf/discretization.py</code> <pre><code>@classmethod\ndef from_grid(cls, grid_values, domain):\n  r\"\"\"Creates an OnGrid field from a grid of values.\n\n      Args:\n        grid_values (ndarray): The grid of values.\n        domain (Domain): The domain of the discretization.\n      \"\"\"\n  a = cls(grid_values, domain)\n  return a\n</code></pre>"},{"location":"discretization.html#jaxdf.discretization.OnGrid.replace_params","title":"<code>replace_params(new_params)</code>","text":"<p>Replaces the parameters of the discretization with new ones. The domain is not changed.</p> <p>Parameters:</p> Name Type Description Default <code>new_params</code> <code>PyTree</code> <p>The new parameters of the discretization.</p> required <p>Returns:</p> Name Type Description <code>OnGrid</code> <p>A linear discretization with the new parameters.</p> Source code in <code>jaxdf/discretization.py</code> <pre><code>def replace_params(self, new_params):\n  r\"\"\"Replaces the parameters of the discretization with new ones. The domain\n      is not changed.\n\n      Args:\n        new_params (PyTree): The new parameters of the discretization.\n\n      Returns:\n        OnGrid: A linear discretization with the new parameters.\n      \"\"\"\n  return self.__class__(new_params, self.domain)\n</code></pre>"},{"location":"geometry.html","title":"geometry","text":""},{"location":"geometry.html#jaxdf.geometry.Domain","title":"<code>Domain</code>","text":"<p>             Bases: <code>Module</code></p> <p>Domain class describing a rectangular domain</p> <p>Attributes:</p> Name Type Description <code>size</code> <code>Tuple[int]</code> <p>The size of the domain in absolute units.</p> <code>dx</code> <code>Tuple(float</code> <p>The unit of measure</p> Source code in <code>jaxdf/geometry.py</code> <pre><code>class Domain(Module):\n  r\"\"\"Domain class describing a rectangular domain\n\n    Attributes:\n        size (Tuple[int]): The size of the domain in absolute units.\n        dx (Tuple(float)): The unit of measure\n    \"\"\"\n  N: Iterable[int] = eqx.field(default=(32, 32), static=True)\n  dx: Iterable[float] = eqx.field(default=(1.0, 1.0), static=True)\n\n  @property\n  def size(self):\n    r\"\"\"The lenght of the grid sides\n\n        Returns:\n            Tuple[float]: The size of the domain, in absolute units\n\n        \"\"\"\n    return list(map(lambda x: x[0] * x[1], zip(self.N, self.dx)))\n\n  @property\n  def ndim(self):\n    r\"\"\"The number of dimensions of the domain\n\n        Returns:\n            int: The number of dimensions of the domain\n\n        \"\"\"\n    return len(self.N)\n\n  @property\n  def cell_volume(self):\n    r\"\"\"The volume of a single cell\n\n        Returns:\n            float: The volume of a single cell\n\n        \"\"\"\n    return reduce(lambda x, y: x * y, self.dx)\n\n  @property\n  def spatial_axis(self):\n    r\"\"\"The spatial axis of the domain\n\n        Returns:\n            Tuple[jnp.array]: The spatial axis of the domain\n\n        \"\"\"\n\n    def _make_axis(n, delta):\n      if n % 2 == 0:\n        return jnp.arange(0, n) * delta - delta * n / 2\n      else:\n        return jnp.arange(0, n) * delta - delta * (n - 1) / 2\n\n    axis = [_make_axis(n, delta) for n, delta in zip(self.N, self.dx)]\n    axis = [ax - jnp.mean(ax) for ax in axis]\n    return axis\n\n  @property\n  def boundary_sampler(self):\n    r\"\"\"Returns a function that samples a point on the boundary of the domain\n\n        Returns:\n            Callable: A function that samples a point on the boundary of the domain.\n                This function takes a seed and an integer number of samples and returns\n                a list of samples.\n\n        !!! example\n            ```python\n            &gt;&gt;&gt; domain = Domain((10, 10), (0.1, 0.1))\n            &gt;&gt;&gt; boundary_sampler = domain.boundary_sampler\n            &gt;&gt;&gt; boundary_sampler(random.PRNGKey(0), 10)\n            Array([[-0.02072918,  0.5       ],\n                   [-0.5       ,  0.49063694],\n                   [-0.18872023, -0.5       ],\n                   [ 0.31801188, -0.5       ],\n                   [-0.1319474 , -0.5       ],\n                   [ 0.5       , -0.36944878],\n                   [ 0.5       ,  0.46956718],\n                   [ 0.4608934 , -0.5       ],\n                   [-0.09031796,  0.5       ],\n                   [-0.5       ,  0.40659428]], dtype=float32)\n\n            ```\n\n        \"\"\"\n    L = jnp.asarray(self.size) / 2\n\n    def sample(seed):\n      seeds = random.split(seed, 3)\n      first = 2 * jnp.expand_dims(random.uniform(seeds[0]), 0) - 1\n      others = 2 * random.bernoulli(seeds[1], shape=(self.ndim - 1, )) - 1\n      sample = jnp.concatenate([first, others]).astype(jnp.float32)\n      random_perm = random.permutation(seeds[2], sample)\n      sample = random_perm * L\n      return sample\n\n    def multi_samples(seed, num_samples: int):\n      seeds = random.split(seed, num_samples)\n      return jax.vmap(sample)(seeds)\n\n    return multi_samples\n\n  @property\n  def domain_sampler(self):\n    r\"\"\"Returns a function that samples a point in the domain\n\n        Returns:\n            Callable: A function that samples a point in the domain.\n                This function takes a seed and an integer number of samples and returns\n                a list of samples.\n\n        !!! example\n            ```python\n            &gt;&gt;&gt; domain = Domain((10, 10), (0.1, 0.1))\n            &gt;&gt;&gt; domain_sampler = domain.domain_sampler\n            &gt;&gt;&gt; domain_sampler(random.PRNGKey(0), 10)\n            Array([[ 0.06298566,  0.35970068],\n                   [-0.20049798,  0.05455852],\n                   [ 0.33402848, -0.04824698],\n                   [ 0.27945423,  0.2805649 ],\n                   [ 0.49464726,  0.3473643 ],\n                   [-0.16299951, -0.27665186],\n                   [-0.06442916,  0.04995835],\n                   [ 0.05011427, -0.17267668],\n                   [-0.39805043, -0.05386746],\n                   [ 0.46900105,  0.21520817]], dtype=float32)\n\n            ```\n\n        \"\"\"\n    L = jnp.asarray(self.size) / 2\n\n    def sample(seed):\n      sample = 2 * random.uniform(seed, shape=(self.ndim, )) - 1\n      return sample * L\n\n    def multi_samples(seed, num_samples: int):\n      seeds = random.split(seed, num_samples)\n      return jax.vmap(sample)(seeds)\n\n    return multi_samples\n\n  @property\n  def origin(self):\n    return jnp.zeros((self.ndim, ))\n\n  @staticmethod\n  def _make_grid_from_axis(axis):\n    return jnp.stack(jnp.meshgrid(*axis, indexing=\"ij\"), axis=-1)\n\n  @property\n  def grid(self):\n    \"\"\"A grid of spatial position, of size\n        `Nx x Ny x Nz x ... x num_axis` such that the element\n        `[x1,x2,x3, .., :]` is a coordinate vector.\n\n        Returns:\n            jnp.array: A grid of spatial position\n\n        \"\"\"\n    axis = self.spatial_axis\n    return self._make_grid_from_axis(axis)\n</code></pre>"},{"location":"geometry.html#jaxdf.geometry.Domain.boundary_sampler","title":"<code>boundary_sampler</code>  <code>property</code>","text":"<p>Returns a function that samples a point on the boundary of the domain</p> <p>Returns:</p> Name Type Description <code>Callable</code> <p>A function that samples a point on the boundary of the domain. This function takes a seed and an integer number of samples and returns a list of samples.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; domain = Domain((10, 10), (0.1, 0.1))\n&gt;&gt;&gt; boundary_sampler = domain.boundary_sampler\n&gt;&gt;&gt; boundary_sampler(random.PRNGKey(0), 10)\nArray([[-0.02072918,  0.5       ],\n       [-0.5       ,  0.49063694],\n       [-0.18872023, -0.5       ],\n       [ 0.31801188, -0.5       ],\n       [-0.1319474 , -0.5       ],\n       [ 0.5       , -0.36944878],\n       [ 0.5       ,  0.46956718],\n       [ 0.4608934 , -0.5       ],\n       [-0.09031796,  0.5       ],\n       [-0.5       ,  0.40659428]], dtype=float32)\n</code></pre>"},{"location":"geometry.html#jaxdf.geometry.Domain.cell_volume","title":"<code>cell_volume</code>  <code>property</code>","text":"<p>The volume of a single cell</p> <p>Returns:</p> Name Type Description <code>float</code> <p>The volume of a single cell</p>"},{"location":"geometry.html#jaxdf.geometry.Domain.domain_sampler","title":"<code>domain_sampler</code>  <code>property</code>","text":"<p>Returns a function that samples a point in the domain</p> <p>Returns:</p> Name Type Description <code>Callable</code> <p>A function that samples a point in the domain. This function takes a seed and an integer number of samples and returns a list of samples.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; domain = Domain((10, 10), (0.1, 0.1))\n&gt;&gt;&gt; domain_sampler = domain.domain_sampler\n&gt;&gt;&gt; domain_sampler(random.PRNGKey(0), 10)\nArray([[ 0.06298566,  0.35970068],\n       [-0.20049798,  0.05455852],\n       [ 0.33402848, -0.04824698],\n       [ 0.27945423,  0.2805649 ],\n       [ 0.49464726,  0.3473643 ],\n       [-0.16299951, -0.27665186],\n       [-0.06442916,  0.04995835],\n       [ 0.05011427, -0.17267668],\n       [-0.39805043, -0.05386746],\n       [ 0.46900105,  0.21520817]], dtype=float32)\n</code></pre>"},{"location":"geometry.html#jaxdf.geometry.Domain.grid","title":"<code>grid</code>  <code>property</code>","text":"<p>A grid of spatial position, of size <code>Nx x Ny x Nz x ... x num_axis</code> such that the element <code>[x1,x2,x3, .., :]</code> is a coordinate vector.</p> <p>Returns:</p> Type Description <p>jnp.array: A grid of spatial position</p>"},{"location":"geometry.html#jaxdf.geometry.Domain.ndim","title":"<code>ndim</code>  <code>property</code>","text":"<p>The number of dimensions of the domain</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The number of dimensions of the domain</p>"},{"location":"geometry.html#jaxdf.geometry.Domain.size","title":"<code>size</code>  <code>property</code>","text":"<p>The lenght of the grid sides</p> <p>Returns:</p> Type Description <p>Tuple[float]: The size of the domain, in absolute units</p>"},{"location":"geometry.html#jaxdf.geometry.Domain.spatial_axis","title":"<code>spatial_axis</code>  <code>property</code>","text":"<p>The spatial axis of the domain</p> <p>Returns:</p> Type Description <p>Tuple[jnp.array]: The spatial axis of the domain</p>"},{"location":"macros.html","title":"Macros","text":"In\u00a0[\u00a0]: Copied! <pre>from plumkdocs import define_env\n</pre> from plumkdocs import define_env In\u00a0[\u00a0]: Copied! <pre>__all__ = [\"define_env\"]\n</pre> __all__ = [\"define_env\"]"},{"location":"mods.html","title":"mods","text":""},{"location":"mods.html#jaxdf.mods.Module","title":"<code>Module</code>","text":"<p>             Bases: <code>Module</code></p> <p>A custom module inheriting from Equinox's Module class.</p> Source code in <code>jaxdf/mods.py</code> <pre><code>class Module(eqx.Module):\n  \"\"\"\n    A custom module inheriting from Equinox's Module class.\n    \"\"\"\n\n  def replace(self, name: str, value: PyTree):\n    \"\"\"\n        Replaces the attribute of the module with the given name with a new value.\n\n        This method utilizes `eqx.tree_at` to update the attribute in a functional\n        manner, ensuring compatibility with JAX's functional approach and autodiff capabilities.\n\n        Args:\n            name (str): The name of the attribute to be replaced.\n            value (PyTree): The new value to set for the attribute. This should be\n                            compatible with JAX's PyTree structure.\n\n        Returns:\n            A new instance of Module with the specified attribute updated.\n            The rest of the module's attributes remain unchanged.\n\n        !!! example\n        ```python\n            &gt;&gt;&gt; module = jaxdf.Module(weight=1.0, bias=2.0)\n            &gt;&gt;&gt; new_module = module.replace(\"weight\", 3.0)\n            &gt;&gt;&gt; new_module.weight == 3.0    # True\n        ```\n        \"\"\"\n    f = lambda m: m.__getattribute__(name)\n    return eqx.tree_at(f, self, value)\n</code></pre>"},{"location":"mods.html#jaxdf.mods.Module.replace","title":"<code>replace(name, value)</code>","text":"<p>Replaces the attribute of the module with the given name with a new value.</p> <p>This method utilizes <code>eqx.tree_at</code> to update the attribute in a functional manner, ensuring compatibility with JAX's functional approach and autodiff capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the attribute to be replaced.</p> required <code>value</code> <code>PyTree</code> <p>The new value to set for the attribute. This should be             compatible with JAX's PyTree structure.</p> required <p>Returns:</p> Type Description <p>A new instance of Module with the specified attribute updated.</p> <p>The rest of the module's attributes remain unchanged.</p> <p>Example</p> <pre><code>    &gt;&gt;&gt; module = jaxdf.Module(weight=1.0, bias=2.0)\n    &gt;&gt;&gt; new_module = module.replace(\"weight\", 3.0)\n    &gt;&gt;&gt; new_module.weight == 3.0    # True\n</code></pre> Source code in <code>jaxdf/mods.py</code> <pre><code>def replace(self, name: str, value: PyTree):\n  \"\"\"\n      Replaces the attribute of the module with the given name with a new value.\n\n      This method utilizes `eqx.tree_at` to update the attribute in a functional\n      manner, ensuring compatibility with JAX's functional approach and autodiff capabilities.\n\n      Args:\n          name (str): The name of the attribute to be replaced.\n          value (PyTree): The new value to set for the attribute. This should be\n                          compatible with JAX's PyTree structure.\n\n      Returns:\n          A new instance of Module with the specified attribute updated.\n          The rest of the module's attributes remain unchanged.\n\n      !!! example\n      ```python\n          &gt;&gt;&gt; module = jaxdf.Module(weight=1.0, bias=2.0)\n          &gt;&gt;&gt; new_module = module.replace(\"weight\", 3.0)\n          &gt;&gt;&gt; new_module.weight == 3.0    # True\n      ```\n      \"\"\"\n  f = lambda m: m.__getattribute__(name)\n  return eqx.tree_at(f, self, value)\n</code></pre>"},{"location":"util.html","title":"util","text":""},{"location":"util.html#jaxdf.util.get_implemented","title":"<code>get_implemented(f)</code>","text":"<p>Prints the implemented methods of an operator</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>The operator to get the implemented methods of.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>jaxdf/util.py</code> <pre><code>def get_implemented(f):\n  r\"\"\"Prints the implemented methods of an operator\n\n    Arguments:\n      f (Callable): The operator to get the implemented methods of.\n\n    Returns:\n      None\n\n    \"\"\"\n\n  # TODO: Why there are more instances for the same types?\n\n  print(f.__name__ + \":\")\n  instances = []\n  a = f.methods\n  for f_instance in a:\n    # Get types\n    types = f_instance.signature.types\n\n    # Change each type with its classname\n    types = tuple(map(lambda x: x.__name__, types))\n\n    # Append\n    instances.append(str(types))\n\n  instances = set(instances)\n  for instance in instances:\n    print(\" \u2500 \" + instance)\n</code></pre>"},{"location":"util.html#jaxdf.util.update_dictionary","title":"<code>update_dictionary(old, new_entries)</code>","text":"<p>Update a dictionary with new entries.</p> <p>Parameters:</p> Name Type Description Default <code>old</code> <code>dict</code> <p>The dictionary to update</p> required <code>new_entries</code> <code>dict</code> <p>The new entries to add to the dictionary</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The updated dictionary</p> Source code in <code>jaxdf/util.py</code> <pre><code>def update_dictionary(old: dict, new_entries: dict):\n  r\"\"\"Update a dictionary with new entries.\n\n    Args:\n      old (dict): The dictionary to update\n      new_entries (dict): The new entries to add to the dictionary\n\n    Returns:\n      dict: The updated dictionary\n    \"\"\"\n  for key, val in zip(new_entries.keys(), new_entries.values()):\n    old[key] = val\n  return old\n</code></pre>"},{"location":"notebooks/api_discretization.html","title":"Discretization API","text":"In\u00a0[1]: Copied! <pre>from jaxdf.discretization import Continuous\nfrom jaxdf.geometry import Domain\nfrom jax.random import normal, PRNGKey\nfrom jax import numpy as jnp\n\nseed = PRNGKey(42)\nN = 5\n\n# This defines the spatial domain of the function\ndomain = Domain((256,),(1/64.,))\n\n# This is the mapping from the parameters to the function\ndef p_n(theta, x):\n    i = jnp.arange(N)\n    powers = x**i\n    return jnp.expand_dims(jnp.sum(theta*(x**i)), -1)\n\n# Here we generate a random set of parameters\nparams = normal(seed, (N,))\n\n# Finally, we place them all together in a single object\n# that can be used to evaluate the function and apply operators\n# to it.\nu = Continuous(params, domain, p_n)\n\nprint(u)\n</pre> from jaxdf.discretization import Continuous from jaxdf.geometry import Domain from jax.random import normal, PRNGKey from jax import numpy as jnp  seed = PRNGKey(42) N = 5  # This defines the spatial domain of the function domain = Domain((256,),(1/64.,))  # This is the mapping from the parameters to the function def p_n(theta, x):     i = jnp.arange(N)     powers = x**i     return jnp.expand_dims(jnp.sum(theta*(x**i)), -1)  # Here we generate a random set of parameters params = normal(seed, (N,))  # Finally, we place them all together in a single object # that can be used to evaluate the function and apply operators # to it. u = Continuous(params, domain, p_n)  print(u) <pre>Continuous(\n  params=f32[5],\n  domain=Domain(N=(256,), dx=(0.015625,)),\n  get_fun=&lt;function p_n&gt;\n)\n</pre> <p>Let's look at the field over the domain using the <code>on_grid</code> method</p> In\u00a0[2]: Copied! <pre>from matplotlib import pyplot as plt\n\ndef show_field(grid_representation, domain):\n    plt.plot(domain.spatial_axis[0], grid_representation)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"$f$\")\n    plt.show()\n    \nfield_on_grid = u.on_grid\n\nshow_field(field_on_grid, domain)\n</pre> from matplotlib import pyplot as plt  def show_field(grid_representation, domain):     plt.plot(domain.spatial_axis[0], grid_representation)     plt.xlabel(\"x\")     plt.ylabel(\"$f$\")     plt.show()      field_on_grid = u.on_grid  show_field(field_on_grid, domain) <p>To get the field at a specific location $x$, we can simply call the field with at the required coordinates.</p> In\u00a0[3]: Copied! <pre>x = 1.2\nfield_at_x = u(x)\nprint(f\"Field at x={x} : {field_at_x}\")\n</pre> x = 1.2 field_at_x = u(x) print(f\"Field at x={x} : {field_at_x}\") <pre>Field at x=1.2 : [-2.5217302]\n</pre> <p>Fields are pytrees, and are based on equinox <code>Module</code>, so they natively work <code>jax.jit</code>, <code>jax.grad</code> etc.</p> <p>Let's try to apply derivative operator to this newly defined field</p> In\u00a0[4]: Copied! <pre>from jaxdf.operators import derivative\n\ndu_dx = derivative(u)\n</pre> from jaxdf.operators import derivative  du_dx = derivative(u) In\u00a0[5]: Copied! <pre>show_field(du_dx.on_grid, domain)\n</pre> show_field(du_dx.on_grid, domain) <p>The parameter of the field <code>du_dx</code> are the same as <code>u</code>, since for <code>Continuous</code> fields the <code>gradient</code> operator is evaluated using autograd, which is an operation that only affects the computational grah of a function but not its inputs</p> In\u00a0[6]: Copied! <pre>u.\u03b8, du_dx.\u03b8 # .\u03b8 is a shorthand for .params\n</pre> u.\u03b8, du_dx.\u03b8 # .\u03b8 is a shorthand for .params Out[6]: <pre>(Array([ 0.6122652,  1.1225883, -0.8544134, -0.8127325, -0.890405 ],      dtype=float32),\n Array([ 0.6122652,  1.1225883, -0.8544134, -0.8127325, -0.890405 ],      dtype=float32))</pre> In\u00a0[7]: Copied! <pre>class Polynomial(Continuous):\n    @classmethod\n    def from_params(cls, params, domain):\n        \n        def get_fun(params, x):\n            i = jnp.arange(len(params))\n            return jnp.expand_dims(jnp.sum(params*(x**i)), -1)\n        \n        return cls(\n            params = params, \n            domain = domain, \n            get_fun = get_fun\n        )\n    \n    @property\n    def degree(self):\n        return len(self.params)-1\n    \n    def __repr__(self):\n        return \"Polynomial(degree={})\".format(self.degree)\n</pre> class Polynomial(Continuous):     @classmethod     def from_params(cls, params, domain):                  def get_fun(params, x):             i = jnp.arange(len(params))             return jnp.expand_dims(jnp.sum(params*(x**i)), -1)                  return cls(             params = params,              domain = domain,              get_fun = get_fun         )          @property     def degree(self):         return len(self.params)-1          def __repr__(self):         return \"Polynomial(degree={})\".format(self.degree) In\u00a0[8]: Copied! <pre># Construct a polynomial field from the same parameters as before\nu_custom = Polynomial.from_params(u.params, u.domain)\n\nprint(u_custom, u_custom.params)\nshow_field(u_custom.on_grid, domain)\n</pre> # Construct a polynomial field from the same parameters as before u_custom = Polynomial.from_params(u.params, u.domain)  print(u_custom, u_custom.params) show_field(u_custom.on_grid, domain) <pre>Polynomial(degree=4) [ 0.6122652  1.1225883 -0.8544134 -0.8127325 -0.890405 ]\n</pre> <p>To now define the <code>derivative</code> operator acting on polynomials, we have several options. One is to simply define a python function that generates the <code>Polynomial</code> object resulting from the gradient computation.</p> In\u00a0[9]: Copied! <pre>def derivative(u: Polynomial):\n    # Find the parameters of the polynomial after differentiation\n    coeffs = jnp.arange(1, u.params.shape[0])\n    new_params = u.params[1:]*coeffs\n    \n    # Return a new polynomial with the new parameters\n    return Polynomial.from_params(new_params, u.domain)\n</pre> def derivative(u: Polynomial):     # Find the parameters of the polynomial after differentiation     coeffs = jnp.arange(1, u.params.shape[0])     new_params = u.params[1:]*coeffs          # Return a new polynomial with the new parameters     return Polynomial.from_params(new_params, u.domain) In\u00a0[10]: Copied! <pre>du_custom = derivative(u_custom)\nshow_field(du_custom.on_grid, domain)\nprint(du_custom)  # Note that the degree is one less than before\n</pre> du_custom = derivative(u_custom) show_field(du_custom.on_grid, domain) print(du_custom)  # Note that the degree is one less than before <pre>Polynomial(degree=3)\n</pre> <p>Note that the code is fully differentiable and can be compiled</p> In\u00a0[11]: Copied! <pre>import jax\n\n@jax.jit\ndef f(u):\n    x = derivative(u) + 0.3\n    return x(0.1)\n\nprint(f(u))\n</pre> import jax  @jax.jit def f(u):     x = derivative(u) + 0.3     return x(0.1)  print(f(u)) <pre>[1.223762]\n</pre> <p>However, note that now we have a <code>derivative</code> operator defined for all types, and we get incorrect results for fields that are not Polynomials</p> In\u00a0[12]: Copied! <pre>def sinfun(params, x):\n    theta = jnp.sum(params) # dummy operation to make the parameter vector a scalar\n    y = jnp.sin(theta*x*10)\n    return y\n\nparams = u.params\nsin = Continuous(params, domain, sinfun)\nshow_field(sin.on_grid, domain)\n</pre> def sinfun(params, x):     theta = jnp.sum(params) # dummy operation to make the parameter vector a scalar     y = jnp.sin(theta*x*10)     return y  params = u.params sin = Continuous(params, domain, sinfun) show_field(sin.on_grid, domain) In\u00a0[13]: Copied! <pre>z = derivative(sin)\nshow_field(z.on_grid, domain)\nprint('z: ', z)\n</pre> z = derivative(sin) show_field(z.on_grid, domain) print('z: ', z) <pre>z:  Polynomial(degree=3)\n</pre> In\u00a0[14]: Copied! <pre>class PolynomialWithMethods(Polynomial):\n    \n    @classmethod\n    def from_params(cls, params, domain):\n        \n        def get_fun(params, x):\n            i = jnp.arange(len(params))\n            return jnp.expand_dims(jnp.sum(params*(x**i)), -1)\n        \n        return cls(\n            params = params, \n            domain = domain, \n            get_fun = get_fun\n        )\n    \n    @property\n    def degree(self):\n        return len(self.params)-1\n    \n    def __repr__(self):\n        return \"Polynomial(degree={})\".format(self.degree)\n    \n    # Custom derivative code, with parameters\n    def derivative(self, exponent=1.0):\n        new_params = self.params[1:]*jnp.arange(1, self.params.shape[0])*exponent\n        return PolynomialWithMethods.from_params(new_params, self.domain)\n    \n@jax.jit\ndef g(u, exponent):\n    return u.derivative(exponent) + 3.0\n\nu_2 = PolynomialWithMethods.from_params(u.params, u.domain)\n\nshow_field(g(u_2, 1.3).on_grid, domain)\n</pre> class PolynomialWithMethods(Polynomial):          @classmethod     def from_params(cls, params, domain):                  def get_fun(params, x):             i = jnp.arange(len(params))             return jnp.expand_dims(jnp.sum(params*(x**i)), -1)                  return cls(             params = params,              domain = domain,              get_fun = get_fun         )          @property     def degree(self):         return len(self.params)-1          def __repr__(self):         return \"Polynomial(degree={})\".format(self.degree)          # Custom derivative code, with parameters     def derivative(self, exponent=1.0):         new_params = self.params[1:]*jnp.arange(1, self.params.shape[0])*exponent         return PolynomialWithMethods.from_params(new_params, self.domain)      @jax.jit def g(u, exponent):     return u.derivative(exponent) + 3.0  u_2 = PolynomialWithMethods.from_params(u.params, u.domain)  show_field(g(u_2, 1.3).on_grid, domain) <p>However, it can become cumbersome to deal with many derived methods for different kind of discretizations, especially if one starts to evaluate operators that accept multiple operands with different combinations of discretizations (e.g. dot products, <code>+</code>, heterogeneous differential operators, etc).</p> <p>This problem elegantly resolved in some programming languages usign multiple-dispatch. One of the languages that notably supports multiple dispatch is the Julia language, and I suggest to look at the packages of the SciML echosystem if you are familiar with Julia and or interested in learning this language (those packages look great!).</p> <p>For us sticking with python, <code>jax</code> and <code>jaxdf</code>, here we borrow those ideas using the python multiple dispatch library plum. The <code>jaxdf.operator</code> decorator can be used to define new (parametric) operators using as</p> <pre>@operator\ndef new_operator(x: Polynomial, y: Continuous, *, params=1.0):\n    ... # Any jaxdf or jax-compatible code\n    return Field(...)\n</pre> <p>The input of the fuction can be arbitrary types: if they are fields or any type which is traceable by jax, they will be traced. The function has a reserved, mandatory input keyword <code>params</code>, which is reserved for the parameters of the operator, like the coefficients of the stencil of a differential operator.</p> <p>The output of the function can be any type, including <code>jaxdf.Field</code> or <code>jax.numpy.ndarray</code>.</p> <p>The use of the <code>@operator</code> decorator makes sense when the arguments are defined using type annotation. In that way, we are using the dispatch method of <code>plum</code> to define an implementation of that function which is specific for the annotated types.</p> In\u00a0[15]: Copied! <pre>from jaxdf import operator\n\n@operator\ndef derivative(x: Polynomial, *, axis=None, params=None):\n    print('Applying derivative to a polynomial')\n    if axis is not None:\n        print(\"Warning: axis argument is ignored for polynomials\")\n        \n    new_params = x.params[1:]*jnp.arange(1, u.params.shape[0])\n    return Polynomial.from_params(new_params, u.domain)\n\n@operator\ndef derivative(x: Continuous, *, axis=0, params=None):\n    print('Applying derivative to a generic Continuous field')\n    get_x = x.get_fun\n    def grad_fun(p, coords):\n        f_jac = jax.jacfwd(get_x, argnums=(1,))\n        return jnp.expand_dims(f_jac(p, coords)[0][0][axis], -1)\n    return Continuous(x.params, x.domain, grad_fun)\n</pre> from jaxdf import operator  @operator def derivative(x: Polynomial, *, axis=None, params=None):     print('Applying derivative to a polynomial')     if axis is not None:         print(\"Warning: axis argument is ignored for polynomials\")              new_params = x.params[1:]*jnp.arange(1, u.params.shape[0])     return Polynomial.from_params(new_params, u.domain)  @operator def derivative(x: Continuous, *, axis=0, params=None):     print('Applying derivative to a generic Continuous field')     get_x = x.get_fun     def grad_fun(p, coords):         f_jac = jax.jacfwd(get_x, argnums=(1,))         return jnp.expand_dims(f_jac(p, coords)[0][0][axis], -1)     return Continuous(x.params, x.domain, grad_fun) In\u00a0[16]: Copied! <pre>du_custom = derivative(u_custom)\nshow_field(du_custom.on_grid, domain)\n</pre> du_custom = derivative(u_custom) show_field(du_custom.on_grid, domain) <pre>Applying derivative to a polynomial\n</pre> In\u00a0[17]: Copied! <pre>z = derivative(sin)\n\nshow_field(z.on_grid, domain)\nprint('z: ', z)\n</pre> z = derivative(sin)  show_field(z.on_grid, domain) print('z: ', z) <pre>Applying derivative to a generic Continuous field\n</pre> <pre>z:  Continuous(\n  params=f32[5],\n  domain=Domain(N=(256,), dx=(0.015625,)),\n  get_fun=&lt;function grad_fun&gt;\n)\n</pre>"},{"location":"notebooks/api_discretization.html#discretization-api","title":"Discretization API\u00b6","text":"<p><code>jaxdf</code> revolves around the concept of discretization.</p> <p>We will call discretization family the mapping $\\mathcal{D}$ that associates a function $f$ to a set of discrete parameters $\\theta$</p> <p>$$ \\theta \\xrightarrow{\\mathcal{D}}f $$</p> <p>with $f \\in \\text{Range}(\\mathcal{D})$ or, in other words, $f_\\theta(x) = \\mathcal{D}(\\theta, x)$ is a function parametrized by $\\theta$.</p> <p>$\\theta$ is the discrete representation of $f$ over $\\mathcal{D}$. The latter is analogous to the interpolation function defined in other libraries (See for example the Operator Discretization Library)</p>"},{"location":"notebooks/api_discretization.html#example","title":"Example\u00b6","text":"<p>A simple example of discretization family is the set of $N$-th order polynomials on the interval $[0,1)$:</p> <p>$$ \\mathcal{P}_N(\\theta,x) = \\sum_{i=0}^N \\theta_i x^i, \\qquad \\theta \\in \\mathbb{R}^{N+1} $$</p> <p>In <code>jaxdf</code>, we construct such a field using the <code>Continuous</code> discretization. To do so, we have to provide the function $\\mathcal{P}_N$, the parameters of the function and the domain where the function is defined</p>"},{"location":"notebooks/api_discretization.html#customizing-discretizations","title":"Customizing discretizations\u00b6","text":"<p>For a polynomial field, we actually know analytically how to compute derivatives:</p> <p>$$ \\frac{\\partial}{\\partial x}\\mathcal{P}_N(\\theta,x) = \\frac{\\partial}{\\partial x} \\sum_{i=0}^N \\theta_i x^i = \\sum_{i=0}^{N-1} i\\theta_{i+1} x^i $$</p> <p>To use this knowledge, we first define a new discretization family from the <code>Continuous</code> one, and then we define the <code>gradient</code> method using the analytical formula.</p> <p>This new class needs to initialize the parent class using the <code>super().__init__()</code> method; the input parameters are <code>params,domain,get_field</code>, however we can use knowledge of the formula for the <code>get_fun</code>.</p> <p>The <code>params</code> one must be a <code>PyTree</code> compatible with <code>jax.numpy</code> (arrays, dictionaries of arrays, equinox modules, etc). The <code>domain</code> attribute must be the <code>jaxdf.geometry.Domain</code> object defining the domain of the field. The last attribute, <code>get_field</code>, must be a function that evaluates the field at a coordinate using the parameters contained in <code>params</code>, and has the signature <code>get_fun(params: Field.params, x: Union[jnp.ndarray,float])</code>.</p>"},{"location":"notebooks/api_discretization.html#operators-and-multiple-dispatch","title":"Operators and Multiple Dispatch\u00b6","text":"<p>One way to avoid this is to implement the operators as methods of the fields, which are then redefined by the children classes. This was the approach used in <code>jaxdf</code> when it was first made public, and it still can be used</p>"},{"location":"notebooks/example_1_paper.html","title":"Example 1 paper","text":"In\u00a0[1]: Copied! <pre>from jaxdf.geometry import Domain\nfrom jaxdf.operators import *\nfrom jax import numpy as jnp\nfrom jax import random\nfrom jax.example_libraries import optimizers\nfrom jax.scipy.sparse.linalg import gmres\nimport jax\nfrom tqdm import tqdm\n</pre> from jaxdf.geometry import Domain from jaxdf.operators import * from jax import numpy as jnp from jax import random from jax.example_libraries import optimizers from jax.scipy.sparse.linalg import gmres import jax from tqdm import tqdm In\u00a0[2]: Copied! <pre># Settings\ndomain = Domain(N=(256, 256), dx=(1., 1.))\nseed = jax.random.PRNGKey(42)\n\n# Funcs and operators\ndef pml_absorption(x):\n    alpha = 2.\n    sigma_star = 1.\n    delta_pml = 100.\n    L_half = 128.\n    \n    abs_x = jnp.abs(x)\n    in_pml_amplitude = (jnp.abs(abs_x-delta_pml)/(L_half - delta_pml))**alpha\n    return jnp.where(abs_x &gt; delta_pml, sigma_star*in_pml_amplitude, 0.)\n\ndef gamma(x, omega=1.):\n    y = compose(x)(pml_absorption)\n    return 1./(1. + 1j*y/omega)\n\ndef mod_laplacian(u, pml):\n    # Turning off Nyquist correction, see https://math.mit.edu/~stevenj/fft-deriv.pdf\n    grad_u = gradient(u, correct_nyquist=False)\n    mod_grad_u = grad_u*pml\n    mod_diag_jacobian = diag_jacobian(mod_grad_u, correct_nyquist=False)*pml\n    return sum_over_dims(mod_diag_jacobian)\n\n@jax.jit\ndef helmholtz(u, c, pml, omega=1.):\n    # Get the modified laplacian\n    L = mod_laplacian(u, pml)\n    \n    # Add the wavenumber term\n    k = ((omega/c)**2)*u    \n    return L + k\n\n@jax.jit\ndef total_variation(u):\n    nabla_u = laplacian(u)\n    absval = compose(nabla_u)(jnp.abs)\n    return jnp.mean(absval.on_grid)\n\n# Field parameters\nx = Continuous(None, domain, lambda p, x: x)\npml = gamma(x)\n\npml_on_grid = pml.on_grid\nzero_params = jnp.zeros((256,256,1)) + 0j\nsrc_params = zero_params.at[128,64].set(1.0)\n\n# Defining fields\nu = FourierSeries(zero_params, domain)\npml = FourierSeries(pml_on_grid, domain)\nc =  FourierSeries(zero_params, domain)\nsrc = FourierSeries(src_params, domain)\n</pre> # Settings domain = Domain(N=(256, 256), dx=(1., 1.)) seed = jax.random.PRNGKey(42)  # Funcs and operators def pml_absorption(x):     alpha = 2.     sigma_star = 1.     delta_pml = 100.     L_half = 128.          abs_x = jnp.abs(x)     in_pml_amplitude = (jnp.abs(abs_x-delta_pml)/(L_half - delta_pml))**alpha     return jnp.where(abs_x &gt; delta_pml, sigma_star*in_pml_amplitude, 0.)  def gamma(x, omega=1.):     y = compose(x)(pml_absorption)     return 1./(1. + 1j*y/omega)  def mod_laplacian(u, pml):     # Turning off Nyquist correction, see https://math.mit.edu/~stevenj/fft-deriv.pdf     grad_u = gradient(u, correct_nyquist=False)     mod_grad_u = grad_u*pml     mod_diag_jacobian = diag_jacobian(mod_grad_u, correct_nyquist=False)*pml     return sum_over_dims(mod_diag_jacobian)  @jax.jit def helmholtz(u, c, pml, omega=1.):     # Get the modified laplacian     L = mod_laplacian(u, pml)          # Add the wavenumber term     k = ((omega/c)**2)*u         return L + k  @jax.jit def total_variation(u):     nabla_u = laplacian(u)     absval = compose(nabla_u)(jnp.abs)     return jnp.mean(absval.on_grid)  # Field parameters x = Continuous(None, domain, lambda p, x: x) pml = gamma(x)  pml_on_grid = pml.on_grid zero_params = jnp.zeros((256,256,1)) + 0j src_params = zero_params.at[128,64].set(1.0)  # Defining fields u = FourierSeries(zero_params, domain) pml = FourierSeries(pml_on_grid, domain) c =  FourierSeries(zero_params, domain) src = FourierSeries(src_params, domain) In\u00a0[3]: Copied! <pre># Those are the parameters we want to optimize\nlens_params = random.uniform(seed, (168,40))-4\n\n# Wrap Helmholtz operator around a function to work with GMRES\ndef helm_func(u):\n    return helmholtz(u, c, pml)\n\ndef get_sos(T):\n    lens = jnp.zeros(domain.N)\n    lens = lens.at[44:212,108:148].set(jax.nn.sigmoid(T)) + 1\n    return FourierSeries(jnp.expand_dims(lens, -1), domain)\n\ndef find_field(c, x0):\n    def helm_func(u):\n        return helmholtz(u, c, pml, 0.5)\n    sol, _ = gmres(helm_func, src, x0=x0, maxiter=1000, tol=1e-4, restart=10)\n    return sol\n\ndef loss(p, guess):\n    sos = get_sos(p)\n    tv_term = total_variation(sos)\n    field = find_field(sos, x0=guess)\n    target_val = field.on_grid[70,210,0]\n    return (-jnp.abs(target_val)) + 1e-4*tv_term, field\n</pre> # Those are the parameters we want to optimize lens_params = random.uniform(seed, (168,40))-4  # Wrap Helmholtz operator around a function to work with GMRES def helm_func(u):     return helmholtz(u, c, pml)  def get_sos(T):     lens = jnp.zeros(domain.N)     lens = lens.at[44:212,108:148].set(jax.nn.sigmoid(T)) + 1     return FourierSeries(jnp.expand_dims(lens, -1), domain)  def find_field(c, x0):     def helm_func(u):         return helmholtz(u, c, pml, 0.5)     sol, _ = gmres(helm_func, src, x0=x0, maxiter=1000, tol=1e-4, restart=10)     return sol  def loss(p, guess):     sos = get_sos(p)     tv_term = total_variation(sos)     field = find_field(sos, x0=guess)     target_val = field.on_grid[70,210,0]     return (-jnp.abs(target_val)) + 1e-4*tv_term, field In\u00a0[4]: Copied! <pre>from jax import jit\n\nlosshistory = []\n\ninit_fun, update_fun, get_params = optimizers.adam(.1, b1=0.9, b2=0.9)\nopt_state = init_fun(lens_params)\n\n@jit\ndef update(opt_state, field):\n    loss_and_field, gradient = jax.value_and_grad(loss, has_aux=True)(get_params(opt_state), field)\n    return loss_and_field[0], loss_and_field[1], update_fun(k, gradient, opt_state)\n\npbar = tqdm(range(100))\nfield = -src\nfor k in pbar:\n    lossval, field, opt_state = update(opt_state, field)\n    # For logging\n    pbar.set_description(\"Ampl: {:01.4f}\".format(-lossval))\n    losshistory.append(lossval)\n    \ntransmit_phase = get_params(opt_state)\n</pre> from jax import jit  losshistory = []  init_fun, update_fun, get_params = optimizers.adam(.1, b1=0.9, b2=0.9) opt_state = init_fun(lens_params)  @jit def update(opt_state, field):     loss_and_field, gradient = jax.value_and_grad(loss, has_aux=True)(get_params(opt_state), field)     return loss_and_field[0], loss_and_field[1], update_fun(k, gradient, opt_state)  pbar = tqdm(range(100)) field = -src for k in pbar:     lossval, field, opt_state = update(opt_state, field)     # For logging     pbar.set_description(\"Ampl: {:01.4f}\".format(-lossval))     losshistory.append(lossval)      transmit_phase = get_params(opt_state) <pre>Ampl: 0.1340: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [09:30&lt;00:00,  5.71s/it]\n</pre> In\u00a0[5]: Copied! <pre>from matplotlib import pyplot as plt\n\noptimal_field = field\nsos_opt = get_sos(get_params(opt_state))\ntv_sos = total_variation(sos_opt)\n\nfig, ax = plt.subplots(1,2,figsize=(12,6), gridspec_kw={'width_ratios': [3, 1]})\n\nim1 = ax[0].imshow(jnp.abs(optimal_field.on_grid), cmap=\"inferno\", vmax=.12)\nax[0].axis(\"off\")\nax[0].scatter(210,70, marker='x', color=\"red\", label=\"Target\", alpha=0.8)\nplt.colorbar(im1, ax=ax[0])\nax[0].set_title(\"Wavefield amplitude\")\nax[0].legend(loc=\"lower right\")\n\nsos_opt = get_sos(get_params(opt_state)).on_grid\nim2 = ax[1].imshow(sos_opt[44:212,108:148], cmap=\"binary\", vmin=1, vmax=2)\nplt.colorbar(im2, ax=ax[1])\nax[1].set_title(\"Optimized lens\")\nax[1].axis(\"off\")\n</pre> from matplotlib import pyplot as plt  optimal_field = field sos_opt = get_sos(get_params(opt_state)) tv_sos = total_variation(sos_opt)  fig, ax = plt.subplots(1,2,figsize=(12,6), gridspec_kw={'width_ratios': [3, 1]})  im1 = ax[0].imshow(jnp.abs(optimal_field.on_grid), cmap=\"inferno\", vmax=.12) ax[0].axis(\"off\") ax[0].scatter(210,70, marker='x', color=\"red\", label=\"Target\", alpha=0.8) plt.colorbar(im1, ax=ax[0]) ax[0].set_title(\"Wavefield amplitude\") ax[0].legend(loc=\"lower right\")  sos_opt = get_sos(get_params(opt_state)).on_grid im2 = ax[1].imshow(sos_opt[44:212,108:148], cmap=\"binary\", vmin=1, vmax=2) plt.colorbar(im2, ax=ax[1]) ax[1].set_title(\"Optimized lens\") ax[1].axis(\"off\") Out[5]: <pre>(-0.5, 39.5, 167.5, -0.5)</pre>"},{"location":"notebooks/helmholtz_pinn.html","title":"Solving Helmholtz equation with PINNs","text":"In\u00a0[1]: Copied! <pre>from jaxdf.discretization import Continuous\nfrom jaxdf.geometry import Domain\n\nfrom matplotlib import pyplot as plt\nfrom jax import numpy as jnp\nimport jax\nfrom jax.example_libraries import stax\nfrom jax import random\n\nseed = random.PRNGKey(42)\n</pre> from jaxdf.discretization import Continuous from jaxdf.geometry import Domain  from matplotlib import pyplot as plt from jax import numpy as jnp import jax from jax.example_libraries import stax from jax import random  seed = random.PRNGKey(42) In\u00a0[2]: Copied! <pre>def show_field(u):\n    u = u.on_grid\n    maxval = jnp.amax(jnp.abs(u)) \n    plt.figure(figsize=(8,6))\n    plt.imshow(u, cmap=\"RdBu\", extent=[0,1,-1,1], vmin=-maxval, vmax=maxval, aspect='auto')\n    plt.colorbar()\n    plt.show()\n</pre> def show_field(u):     u = u.on_grid     maxval = jnp.amax(jnp.abs(u))      plt.figure(figsize=(8,6))     plt.imshow(u, cmap=\"RdBu\", extent=[0,1,-1,1], vmin=-maxval, vmax=maxval, aspect='auto')     plt.colorbar()     plt.show() In\u00a0[3]: Copied! <pre>domain = Domain((128, 128), (1, 1))\nomega = .3\n</pre> domain = Domain((128, 128), (1, 1)) omega = .3 In\u00a0[4]: Copied! <pre># Helmholtz functions\ndef sigma(x):\n    alpha = 2.\n    sigma_star = 10.\n    delta_pml = 54.\n    L_half = 64.\n    \n    abs_x = jnp.abs(x)\n    in_pml_amplitude = (jnp.abs(abs_x-delta_pml)/(L_half - delta_pml))**alpha\n    return jnp.where(abs_x &gt; delta_pml, sigma_star*in_pml_amplitude, 0.)\n\ndef gamma(x, omega=1.):\n    y = compose(x)(sigma)\n    return 1./(1. + 1j*y/omega)\n\ndef mod_laplacian(u, x):\n    pml = gamma(x)\n    grad_u = gradient(u)\n    mod_grad_u = grad_u*pml\n    mod_diag_jacobian = diag_jacobian(mod_grad_u)*pml\n    return sum_over_dims(mod_diag_jacobian)\n\ndef helmholtz(u, c, x, omega=omega):\n    # Get the modified laplacian\n    L = mod_laplacian(u, x)\n    \n    # Add the wavenumber term\n    k = ((omega/c)**2)*u    \n    return L + k\n</pre> # Helmholtz functions def sigma(x):     alpha = 2.     sigma_star = 10.     delta_pml = 54.     L_half = 64.          abs_x = jnp.abs(x)     in_pml_amplitude = (jnp.abs(abs_x-delta_pml)/(L_half - delta_pml))**alpha     return jnp.where(abs_x &gt; delta_pml, sigma_star*in_pml_amplitude, 0.)  def gamma(x, omega=1.):     y = compose(x)(sigma)     return 1./(1. + 1j*y/omega)  def mod_laplacian(u, x):     pml = gamma(x)     grad_u = gradient(u)     mod_grad_u = grad_u*pml     mod_diag_jacobian = diag_jacobian(mod_grad_u)*pml     return sum_over_dims(mod_diag_jacobian)  def helmholtz(u, c, x, omega=omega):     # Get the modified laplacian     L = mod_laplacian(u, x)          # Add the wavenumber term     k = ((omega/c)**2)*u         return L + k In\u00a0[5]: Copied! <pre>from jaxdf.operators import compose\n\n# Coordinate field\nx = Continuous(None, domain, lambda p, x: x)\n\nplt.imshow(jnp.real(compose(x)(sigma).on_grid)[...,0])\n</pre> from jaxdf.operators import compose  # Coordinate field x = Continuous(None, domain, lambda p, x: x)  plt.imshow(jnp.real(compose(x)(sigma).on_grid)[...,0]) Out[5]: <pre>&lt;matplotlib.image.AxesImage at 0x7f2030090c40&gt;</pre> In\u00a0[6]: Copied! <pre>from jax.nn.initializers import glorot_normal, normal\n\ndef First(out_dim, W_init=glorot_normal()):\n    \n    def init_fun(rng, input_shape):\n        keys = random.split(rng, 4)\n        projected_shape = input_shape[:-1] + (out_dim,)\n        omega = W_init(keys[2], (input_shape[-1], out_dim))\n        phi =  normal()(keys[3], (out_dim,))\n        \n        output_shape = (projected_shape, input_shape)\n        \n        return output_shape, (omega, phi)\n    \n    def apply_fun(params, z, **kwargs):\n        omega, phi = params\n        freq = jnp.dot(z, omega)\n        y = jnp.sin(freq + phi)\n        return (y, z)\n    \n    return init_fun, apply_fun\n\ndef Middle(W_init=glorot_normal()):\n    \n    def init_fun(rng, input_shape):\n        y_shape, z_shape = input_shape\n        \n        keys = random.split(rng, 4)\n        \n        W = W_init(keys[0], (y_shape[-1], y_shape[-1]))\n        b = normal()(keys[1], (y_shape[-1],))\n        omega = W_init(keys[2], (z_shape[-1], y_shape[-1]))\n        phi =  normal()(keys[3], (y_shape[-1],))\n        \n        return input_shape, (W, omega, b, phi)\n    \n    def apply_fun(params, inputs, **kwargs):\n        W, omega, b, phi = params\n        y, z = inputs\n        y = jnp.dot(y, W)+ b\n        g = jnp.sin(jnp.dot(z, omega)+ phi)\n        return (y*g, z)\n    return init_fun, apply_fun\n        \ndef Final(out_dim, C_init=glorot_normal(), b_init=normal()):\n    def init_fun(rng, input_shape):\n        y_shape, _ = input_shape\n        output_shape = y_shape[:-1] + (out_dim,)\n        \n        k1, k2 = random.split(rng)\n        C = C_init(k1, (y_shape[-1], out_dim))\n        b = b_init(k2, (out_dim,))\n        \n        return output_shape, (C,b)\n    \n    def apply_fun(params, inputs, **kwargs):\n        y, z = inputs\n        C, b = params\n        return jnp.dot(y, C) + b\n    return init_fun, apply_fun\n\ninit_random_params, predict = stax.serial(\n    First(256),\n    Middle(),\n    Middle(),\n    Middle(),\n    Middle(),\n    Middle(),\n    Middle(),\n    Middle(),\n    Final(2)\n)\n\ndef init_params(seed, domain):\n    return init_random_params(seed, (len(domain.N),))[1]\n\ndef get_fun(params, x):\n    x = x\n    p = predict(params, x)\n    return jnp.asarray([p[0] + 1j*p[1]])\n\nu = Continuous.from_function(domain, init_params, get_fun, seed)\n</pre> from jax.nn.initializers import glorot_normal, normal  def First(out_dim, W_init=glorot_normal()):          def init_fun(rng, input_shape):         keys = random.split(rng, 4)         projected_shape = input_shape[:-1] + (out_dim,)         omega = W_init(keys[2], (input_shape[-1], out_dim))         phi =  normal()(keys[3], (out_dim,))                  output_shape = (projected_shape, input_shape)                  return output_shape, (omega, phi)          def apply_fun(params, z, **kwargs):         omega, phi = params         freq = jnp.dot(z, omega)         y = jnp.sin(freq + phi)         return (y, z)          return init_fun, apply_fun  def Middle(W_init=glorot_normal()):          def init_fun(rng, input_shape):         y_shape, z_shape = input_shape                  keys = random.split(rng, 4)                  W = W_init(keys[0], (y_shape[-1], y_shape[-1]))         b = normal()(keys[1], (y_shape[-1],))         omega = W_init(keys[2], (z_shape[-1], y_shape[-1]))         phi =  normal()(keys[3], (y_shape[-1],))                  return input_shape, (W, omega, b, phi)          def apply_fun(params, inputs, **kwargs):         W, omega, b, phi = params         y, z = inputs         y = jnp.dot(y, W)+ b         g = jnp.sin(jnp.dot(z, omega)+ phi)         return (y*g, z)     return init_fun, apply_fun          def Final(out_dim, C_init=glorot_normal(), b_init=normal()):     def init_fun(rng, input_shape):         y_shape, _ = input_shape         output_shape = y_shape[:-1] + (out_dim,)                  k1, k2 = random.split(rng)         C = C_init(k1, (y_shape[-1], out_dim))         b = b_init(k2, (out_dim,))                  return output_shape, (C,b)          def apply_fun(params, inputs, **kwargs):         y, z = inputs         C, b = params         return jnp.dot(y, C) + b     return init_fun, apply_fun  init_random_params, predict = stax.serial(     First(256),     Middle(),     Middle(),     Middle(),     Middle(),     Middle(),     Middle(),     Middle(),     Final(2) )  def init_params(seed, domain):     return init_random_params(seed, (len(domain.N),))[1]  def get_fun(params, x):     x = x     p = predict(params, x)     return jnp.asarray([p[0] + 1j*p[1]])  u = Continuous.from_function(domain, init_params, get_fun, seed)  In\u00a0[7]: Copied! <pre>from jaxdf.operators import *\n</pre> from jaxdf.operators import * In\u00a0[8]: Copied! <pre>show_field(compose(u)(jnp.real))\n</pre> show_field(compose(u)(jnp.real)) In\u00a0[9]: Copied! <pre>coord = jnp.asarray([0. ,0.])\nu(coord).shape\n</pre> coord = jnp.asarray([0. ,0.]) u(coord).shape Out[9]: <pre>(1,)</pre> In\u00a0[10]: Copied! <pre># Narrow gaussian pulse as source\ndef init_params(seed, domain):\n    return {}\n\ndef gaussian_func(params, x):\n    x = x + jnp.asarray([32,32])\n    return jnp.expand_dims(jnp.exp(-jnp.sum(x**2)/8) + 0*1j, -1)*10\n\nsrc = Continuous.from_function(domain, init_params, gaussian_func, seed)\nshow_field(compose(src)(jnp.abs))\n</pre> # Narrow gaussian pulse as source def init_params(seed, domain):     return {}  def gaussian_func(params, x):     x = x + jnp.asarray([32,32])     return jnp.expand_dims(jnp.exp(-jnp.sum(x**2)/8) + 0*1j, -1)*10  src = Continuous.from_function(domain, init_params, gaussian_func, seed) show_field(compose(src)(jnp.abs)) In\u00a0[11]: Copied! <pre># Arbitrary Speed of Sound map\ndef init_params(seed, domain):\n    return {}\n\ndef sos_func(params, x):\n    x = x - jnp.asarray([16,16])\n    x = jnp.exp(-jnp.sum(x**2)/512)\n    x = jnp.where(jnp.abs(x)&gt;0.5, .5, 0.)\n    return jnp.expand_dims(x + 1., -1)\n                  \nc = Continuous.from_function(domain, init_params, sos_func, seed)\nshow_field(c)\n</pre> # Arbitrary Speed of Sound map def init_params(seed, domain):     return {}  def sos_func(params, x):     x = x - jnp.asarray([16,16])     x = jnp.exp(-jnp.sum(x**2)/512)     x = jnp.where(jnp.abs(x)&gt;0.5, .5, 0.)     return jnp.expand_dims(x + 1., -1)                    c = Continuous.from_function(domain, init_params, sos_func, seed) show_field(c) In\u00a0[18]: Copied! <pre>from jax import value_and_grad\n\nboundary_sampler = domain.boundary_sampler\ndomain_sampler = domain.domain_sampler\n\ndef boundary_loss(u, seed, batchsize):\n    coords = boundary_sampler(seed, batchsize)\n    field_val = jax.vmap(u)(coords)\n    r = jnp.mean(jnp.abs(field_val)**2)\n    return r\n\ndef domain_loss(u, seed, batchsize):    \n    coords = domain_sampler(seed, batchsize)\n    Hu = lambda coord: helmholtz(u, c, x)(coord)\n    helm_val = jax.vmap(Hu)(coords)\n    src_val = jax.vmap(src)(coords)\n    r = jnp.mean(jnp.abs(src_val - helm_val)**2)\n    return r\n\nbound_valandgrad = value_and_grad(boundary_loss)\ndomain_valandgrad = value_and_grad(domain_loss)\n</pre> from jax import value_and_grad  boundary_sampler = domain.boundary_sampler domain_sampler = domain.domain_sampler  def boundary_loss(u, seed, batchsize):     coords = boundary_sampler(seed, batchsize)     field_val = jax.vmap(u)(coords)     r = jnp.mean(jnp.abs(field_val)**2)     return r  def domain_loss(u, seed, batchsize):         coords = domain_sampler(seed, batchsize)     Hu = lambda coord: helmholtz(u, c, x)(coord)     helm_val = jax.vmap(Hu)(coords)     src_val = jax.vmap(src)(coords)     r = jnp.mean(jnp.abs(src_val - helm_val)**2)     return r  bound_valandgrad = value_and_grad(boundary_loss) domain_valandgrad = value_and_grad(domain_loss) In\u00a0[22]: Copied! <pre>def mod_laplacian(u, x):\n    pml = gamma(x)\n    grad_u = gradient(u)\n    mod_grad_u = grad_u*pml\n    mod_diag_jacobian = diag_jacobian(mod_grad_u)*pml\n    return sum_over_dims(mod_diag_jacobian)\n\ndef helmholtz(u, c, x, omega=omega):\n    # Get the modified laplacian\n    L = mod_laplacian(u, x)\n    \n    # Add the wavenumber term\n    k = ((omega/c)**2)*u   \n    return L + k\n</pre> def mod_laplacian(u, x):     pml = gamma(x)     grad_u = gradient(u)     mod_grad_u = grad_u*pml     mod_diag_jacobian = diag_jacobian(mod_grad_u)*pml     return sum_over_dims(mod_diag_jacobian)  def helmholtz(u, c, x, omega=omega):     # Get the modified laplacian     L = mod_laplacian(u, x)          # Add the wavenumber term     k = ((omega/c)**2)*u        return L + k In\u00a0[23]: Copied! <pre>def show_field(u):\n    u = u.on_grid\n    maxval = jnp.amax(jnp.abs(u)) \n    plt.figure(figsize=(8,6))\n    plt.imshow(u, cmap=\"RdBu\", extent=[0,1,-1,1], vmin=-maxval/2, vmax=maxval/2, aspect='auto')\n    plt.colorbar()\n    plt.show()\n</pre> def show_field(u):     u = u.on_grid     maxval = jnp.amax(jnp.abs(u))      plt.figure(figsize=(8,6))     plt.imshow(u, cmap=\"RdBu\", extent=[0,1,-1,1], vmin=-maxval/2, vmax=maxval/2, aspect='auto')     plt.colorbar()     plt.show() In\u00a0[24]: Copied! <pre># For logging \nimport wandb\n\ndef log_image(wandb, field, name, step):\n    maxval = jnp.amax(jnp.abs(field)) \n    plt.imshow(field[...,0].real, cmap='RdBu', vmin=-maxval/3, vmax=maxval/3)\n    plt.colorbar()\n    img = wandb.Image(plt)\n    wandb.log({name: img}, step=step)\n    plt.close()\n</pre> # For logging  import wandb  def log_image(wandb, field, name, step):     maxval = jnp.amax(jnp.abs(field))      plt.imshow(field[...,0].real, cmap='RdBu', vmin=-maxval/3, vmax=maxval/3)     plt.colorbar()     img = wandb.Image(plt)     wandb.log({name: img}, step=step)     plt.close() In\u00a0[26]: Copied! <pre># Training loop\nfrom jax.example_libraries import optimizers\nfrom jax import jit\nfrom tqdm import tqdm\nfrom jax import tree_map\n\nlosshistory = []\nbatch_size = 2**9\n\ninit_fun, update_fun, get_params = optimizers.adam(.00001)\nopt_state = init_fun(u)\n\ndef loss(u, seed):\n    seeds = random.split(seed, 2)\n    return 0.01*boundary_loss(u, seeds[0], batch_size) +  domain_loss(u, seeds[1], batch_size)\n\n@jit\ndef update(opt_state, seed, k):\n    u = get_params(opt_state)\n    seeds = random.split(seed, 2)\n    \n    # losses\n    boundary_loss, b_gradient = bound_valandgrad(u, seeds[0], batch_size)\n    domain_loss, d_gradient = domain_valandgrad(u, seeds[1], batch_size)\n    \n    # Add gradients of the parameters\n    gradient_nn = tree_map(lambda x,y: 0.01*x+y, b_gradient, d_gradient)\n    \n    return boundary_loss, domain_loss, update_fun(k, gradient_nn, opt_state)\n\npbar = tqdm(range(1000000))\ntol = 1e-3\nrng = seed\n\nwandb.init(project=\"helmholtz-pinn\")\n\nboundary_loss_h = 0.\ndomain_loss_h = 0.\nfor k in pbar:\n    rng, seed = random.split(rng,2)\n    #seed=rng\n    boundary_loss, domain_loss, opt_state = update(opt_state, seed, k)\n    losshistory.append(domain_loss)\n\n    # For logging\n    boundary_loss_h = boundary_loss_h + boundary_loss\n    domain_loss_h = domain_loss_h + domain_loss\n    if (k+1) % 500 == 0:\n        boundary_loss_h = boundary_loss_h / 500.\n        domain_loss_h = domain_loss_h / 500.\n        wandb.log({'boundary_loss': boundary_loss_h, 'domain_loss': domain_loss_h}, step=k)\n        pbar.set_description(\"B: {:01.4f} | D: {:01.4f}\".format(\n            jnp.log10(boundary_loss), \n            jnp.log10(domain_loss)\n        ))\n        boundary_loss_h = 0.\n        domain_loss_h = 0.\n        \n        # Logging image\n        V = get_params(opt_state).on_grid\n        log_image(wandb, V, \"wavefield\", k)\n</pre> # Training loop from jax.example_libraries import optimizers from jax import jit from tqdm import tqdm from jax import tree_map  losshistory = [] batch_size = 2**9  init_fun, update_fun, get_params = optimizers.adam(.00001) opt_state = init_fun(u)  def loss(u, seed):     seeds = random.split(seed, 2)     return 0.01*boundary_loss(u, seeds[0], batch_size) +  domain_loss(u, seeds[1], batch_size)  @jit def update(opt_state, seed, k):     u = get_params(opt_state)     seeds = random.split(seed, 2)          # losses     boundary_loss, b_gradient = bound_valandgrad(u, seeds[0], batch_size)     domain_loss, d_gradient = domain_valandgrad(u, seeds[1], batch_size)          # Add gradients of the parameters     gradient_nn = tree_map(lambda x,y: 0.01*x+y, b_gradient, d_gradient)          return boundary_loss, domain_loss, update_fun(k, gradient_nn, opt_state)  pbar = tqdm(range(1000000)) tol = 1e-3 rng = seed  wandb.init(project=\"helmholtz-pinn\")  boundary_loss_h = 0. domain_loss_h = 0. for k in pbar:     rng, seed = random.split(rng,2)     #seed=rng     boundary_loss, domain_loss, opt_state = update(opt_state, seed, k)     losshistory.append(domain_loss)      # For logging     boundary_loss_h = boundary_loss_h + boundary_loss     domain_loss_h = domain_loss_h + domain_loss     if (k+1) % 500 == 0:         boundary_loss_h = boundary_loss_h / 500.         domain_loss_h = domain_loss_h / 500.         wandb.log({'boundary_loss': boundary_loss_h, 'domain_loss': domain_loss_h}, step=k)         pbar.set_description(\"B: {:01.4f} | D: {:01.4f}\".format(             jnp.log10(boundary_loss),              jnp.log10(domain_loss)         ))         boundary_loss_h = 0.         domain_loss_h = 0.                  # Logging image         V = get_params(opt_state).on_grid         log_image(wandb, V, \"wavefield\", k)  <pre>  0%|                                                                                            | 0/1000000 [00:00&lt;?, ?it/s]</pre>  Finishing last run (ID:8bxjtlug) before initializing another...   Waiting for W&amp;B process to finish... (success). <pre>VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max\u2026</pre>   View run jolly-surf-118 at: https://wandb.ai/bug_ucl/helmholtz-pinn/runs/8bxjtlugSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)   Find logs at: <code>./wandb/run-20230510_221318-8bxjtlug/logs</code>  Successfully finished last run (ID:8bxjtlug). Initializing new run: <pre>VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667072113341419, max=1.0)\u2026</pre>  Tracking run with wandb version 0.15.2   Run data is saved locally in <code>/home/astanziola/repos/jaxdf/docs/notebooks/wandb/run-20230510_221343-iaf6uu7l</code>  Syncing run ethereal-lake-119 to Weights &amp; Biases (docs)   View project at https://wandb.ai/bug_ucl/helmholtz-pinn   View run at https://wandb.ai/bug_ucl/helmholtz-pinn/runs/iaf6uu7l <pre>  0%|                                                                               | 1/1000000 [00:13&lt;3738:52:36, 13.46s/it]/home/astanziola/repos/jaxdf/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2088: UserWarning: Run (8bxjtlug) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n  lambda data: self._console_raw_callback(\"stderr\", data),\nB: -3.4768 | D: -2.7340:  11%|\u2588\u2588\u2588\u2588\u2588\u258b                                             | 112415/1000000 [14:00&lt;1:50:35, 133.77it/s]\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[26], line 42\n     40 rng, seed = random.split(rng,2)\n     41 #seed=rng\n---&gt; 42 boundary_loss, domain_loss, opt_state = update(opt_state, seed, k)\n     43 losshistory.append(domain_loss)\n     45 # For logging\n\nFile ~/repos/jaxdf/.venv/lib/python3.9/site-packages/jax/example_libraries/optimizers.py:120, in &lt;lambda&gt;(data, xs)\n    108 # The implementation here basically works by flattening pytrees. There are two\n    109 # levels of pytrees to think about: the pytree of params, which we can think of\n    110 # as defining an \"outer pytree\", and a pytree produced by applying init_fun to\n    111 # each leaf of the params pytree, which we can think of as the \"inner pytrees\".\n    112 # Since pytrees can be flattened, that structure is isomorphic to a list of\n    113 # lists (with no further nesting).\n    115 OptimizerState = namedtuple(\"OptimizerState\",\n    116                             [\"packed_state\", \"tree_def\", \"subtree_defs\"])\n    117 register_pytree_node(\n    118     OptimizerState,\n    119     lambda xs: ((xs.packed_state,), (xs.tree_def, xs.subtree_defs)),\n--&gt; 120     lambda data, xs: OptimizerState(xs[0], data[0], data[1]))  # type: ignore[index]\n    123 Array = Any\n    124 Params = Any  # Parameters are arbitrary nests of `jnp.ndarrays`.\n\nKeyboardInterrupt: </pre> In\u00a0[35]: Copied! <pre>u = get_params(opt_state)\n\nshow_field(compose(u)(jnp.real))\n</pre> u = get_params(opt_state)  show_field(compose(u)(jnp.real))"},{"location":"notebooks/helmholtz_pinn.html#solving-helmholtz-equation-with-pinns","title":"Solving Helmholtz equation with PINNs\u00b6","text":""},{"location":"notebooks/pinn_burgers.html","title":"Physics informed neural networks","text":"In\u00a0[3]: Copied! <pre>from matplotlib import pyplot as plt\n\nfrom jaxdf.geometry import Domain\nfrom jax import numpy as jnp\nimport jax\n</pre> from matplotlib import pyplot as plt  from jaxdf.geometry import Domain from jax import numpy as jnp import jax In\u00a0[4]: Copied! <pre>domain = Domain(N=(200, 1000), dx=(0.01, 0.001)) # Domain is [space, time]\n</pre> domain = Domain(N=(200, 1000), dx=(0.01, 0.001)) # Domain is [space, time] In\u00a0[5]: Copied! <pre># Get samples for enforcing losses\ndomain_sampler = domain.domain_sampler\nboundary_samples = domain.boundary_sampler\n</pre> # Get samples for enforcing losses domain_sampler = domain.domain_sampler boundary_samples = domain.boundary_sampler In\u00a0[6]: Copied! <pre># Show example\nboundary_sampler = domain.boundary_sampler\nx = boundary_sampler(jax.random.PRNGKey(42), 128)\nplt.scatter(x[:,1],x[:,0], marker='x', label=\"boundary\")\nx = domain_sampler(jax.random.PRNGKey(42), 128)\nplt.scatter(x[:,1],x[:,0], marker='o', label=\"domain\")\nplt.legend()\nplt.title(\"Provided samplers\")\nplt.show()\n</pre> # Show example boundary_sampler = domain.boundary_sampler x = boundary_sampler(jax.random.PRNGKey(42), 128) plt.scatter(x[:,1],x[:,0], marker='x', label=\"boundary\") x = domain_sampler(jax.random.PRNGKey(42), 128) plt.scatter(x[:,1],x[:,0], marker='o', label=\"domain\") plt.legend() plt.title(\"Provided samplers\") plt.show() In\u00a0[7]: Copied! <pre># Construct the PINN\nfrom jaxdf.discretization import Continuous\nfrom jax.example_libraries import stax\nfrom jax import random\n\nseed = random.PRNGKey(0)\n\n# Make a neural network for the field discretization\nTanh = stax.elementwise(jnp.tanh)\nLayers = [stax.Dense(20), Tanh]*9\ninit_random_params, predict_p = stax.serial(*Layers, stax.Dense(1))\n\ndef init_params(seed, domain):\n    return init_random_params(seed, (len(domain.N),))[1]\n\ndef get_fun(params, x):\n    return predict_p(params, x)\n\nu = Continuous.from_function(domain, init_params, get_fun, seed)\n</pre> # Construct the PINN from jaxdf.discretization import Continuous from jax.example_libraries import stax from jax import random  seed = random.PRNGKey(0)  # Make a neural network for the field discretization Tanh = stax.elementwise(jnp.tanh) Layers = [stax.Dense(20), Tanh]*9 init_random_params, predict_p = stax.serial(*Layers, stax.Dense(1))  def init_params(seed, domain):     return init_random_params(seed, (len(domain.N),))[1]  def get_fun(params, x):     return predict_p(params, x)  u = Continuous.from_function(domain, init_params, get_fun, seed) In\u00a0[8]: Copied! <pre>u(jnp.asarray([0.,0.]))\n</pre> u(jnp.asarray([0.,0.])) Out[8]: <pre>DeviceArray([-0.08587985], dtype=float32)</pre> In\u00a0[14]: Copied! <pre>def show_field(u):\n    plt.figure(figsize=(15,4))\n    plt.imshow(u.on_grid, cmap=\"RdBu\", extent=[0,1,-1,1], vmin=-1., vmax=1., aspect='auto')\n    plt.xlabel(\"$t$\")\n    plt.ylabel(\"$x$\")\n    plt.colorbar()\n    plt.show()\n</pre> def show_field(u):     plt.figure(figsize=(15,4))     plt.imshow(u.on_grid, cmap=\"RdBu\", extent=[0,1,-1,1], vmin=-1., vmax=1., aspect='auto')     plt.xlabel(\"$t$\")     plt.ylabel(\"$x$\")     plt.colorbar()     plt.show() In\u00a0[15]: Copied! <pre>show_field(u)\n</pre> show_field(u) In\u00a0[16]: Copied! <pre>from jaxdf.operators.differential import derivative\n\ndef burgers(u):\n    du_dt = derivative(u, axis=1)\n    du_dx = derivative(u, axis=0)\n    ddu_dx = derivative(du_dx, axis=0)\n    return (-0.01/jnp.pi)*ddu_dx + u*du_dx + du_dt\n</pre> from jaxdf.operators.differential import derivative  def burgers(u):     du_dt = derivative(u, axis=1)     du_dx = derivative(u, axis=0)     ddu_dx = derivative(du_dx, axis=0)     return (-0.01/jnp.pi)*ddu_dx + u*du_dx + du_dt In\u00a0[18]: Copied! <pre>sample_burgers = burgers(u)\nshow_field(sample_burgers)\n</pre> sample_burgers = burgers(u) show_field(sample_burgers) In\u00a0[19]: Copied! <pre>from functools import partial\n\nbatchsize = 256\n\n@partial(jax.jit, static_argnums=2)\ndef boundary_loss(u, seed, batchsize):\n    x = domain_sampler(seed, batchsize)\n        \n    # Initial conditions\n    y = x.at[:,1].set(-0.5)\n    \n    # Can apply vmap on a wrapped function of the field\n    @jax.vmap\n    def field_fun(x):\n        return u.get_field(x)\n    \n    field_val = field_fun(y)\n    target_val = jnp.expand_dims(-jnp.sin(jnp.pi*x[:,0]), -1)\n    \n    l1 = jnp.mean(jnp.abs(target_val - field_val)**2)\n    \n    # Boundaries\n    y = x.at[:,0].set(random.bernoulli(seed, shape=(batchsize,))*2 - 1)\n    field_val = field_fun(y)\n    target_val = 0\n    l2 = jnp.mean(jnp.abs(target_val - field_val)**2)\n    \n    return (l1 + l2)/2\n\n\n@partial(jax.jit, static_argnums=2)\ndef domain_loss(u, seed, batchsize):\n    x = domain_sampler(seed, batchsize)\n    \n    @jax.vmap\n    def residual_fun(x):\n        return burgers(u)(x)\n    \n    residual = residual_fun(x)\n    return jnp.mean(jnp.abs(residual)**2)\n\n# Testing losses\nbl = boundary_loss(u, seed, 32)\ndl = domain_loss(u, seed, 32)\n\nprint(f\"Domain loss: {dl.item()}, Boundary loss: {bl.item()}\")\n</pre> from functools import partial  batchsize = 256  @partial(jax.jit, static_argnums=2) def boundary_loss(u, seed, batchsize):     x = domain_sampler(seed, batchsize)              # Initial conditions     y = x.at[:,1].set(-0.5)          # Can apply vmap on a wrapped function of the field     @jax.vmap     def field_fun(x):         return u.get_field(x)          field_val = field_fun(y)     target_val = jnp.expand_dims(-jnp.sin(jnp.pi*x[:,0]), -1)          l1 = jnp.mean(jnp.abs(target_val - field_val)**2)          # Boundaries     y = x.at[:,0].set(random.bernoulli(seed, shape=(batchsize,))*2 - 1)     field_val = field_fun(y)     target_val = 0     l2 = jnp.mean(jnp.abs(target_val - field_val)**2)          return (l1 + l2)/2   @partial(jax.jit, static_argnums=2) def domain_loss(u, seed, batchsize):     x = domain_sampler(seed, batchsize)          @jax.vmap     def residual_fun(x):         return burgers(u)(x)          residual = residual_fun(x)     return jnp.mean(jnp.abs(residual)**2)  # Testing losses bl = boundary_loss(u, seed, 32) dl = domain_loss(u, seed, 32)  print(f\"Domain loss: {dl.item()}, Boundary loss: {bl.item()}\") <pre>Domain loss: 0.021926268935203552, Boundary loss: 0.17989441752433777\n</pre> In\u00a0[20]: Copied! <pre>@jax.jit\ndef update(opt_state, seed, k):\n    seeds = random.split(seed, 2)\n    \n    def tot_loss(u):\n        l1 = domain_loss(u, seeds[0], batchsize)\n        l2 = boundary_loss(u, seeds[1], batchsize)\n        return l1+l2\n    \n    lossval, grads = jax.value_and_grad(tot_loss)(get_params(opt_state))\n    \n    return lossval, update_fun(k, grads, opt_state)\n</pre> @jax.jit def update(opt_state, seed, k):     seeds = random.split(seed, 2)          def tot_loss(u):         l1 = domain_loss(u, seeds[0], batchsize)         l2 = boundary_loss(u, seeds[1], batchsize)         return l1+l2          lossval, grads = jax.value_and_grad(tot_loss)(get_params(opt_state))          return lossval, update_fun(k, grads, opt_state) In\u00a0[21]: Copied! <pre>from jax.example_libraries import optimizers\n\n#Initial field\nu = Continuous.from_function(domain, init_params, get_fun, seed)\n\ninit_fun, update_fun, get_params = optimizers.adam(.0001)\nopt_state = init_fun(u)\n</pre> from jax.example_libraries import optimizers  #Initial field u = Continuous.from_function(domain, init_params, get_fun, seed)  init_fun, update_fun, get_params = optimizers.adam(.0001) opt_state = init_fun(u) In\u00a0[22]: Copied! <pre>from tqdm import tqdm\n\npbar = tqdm(range(100000))\nrng = seed\n\nfor k in pbar:\n    rng, seed = random.split(rng,2)\n    lossval, opt_state = update(opt_state, seed, k)\n    \n    if k % 500 == 99:\n        pbar.set_description(f\"Loss: {lossval}\")\n</pre> from tqdm import tqdm  pbar = tqdm(range(100000)) rng = seed  for k in pbar:     rng, seed = random.split(rng,2)     lossval, opt_state = update(opt_state, seed, k)          if k % 500 == 99:         pbar.set_description(f\"Loss: {lossval}\") <pre>Loss: 0.005336431786417961: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100000/100000 [02:50&lt;00:00, 587.08it/s]\n</pre> In\u00a0[23]: Copied! <pre>u = get_params(opt_state)\n\nshow_field(u)\n</pre> u = get_params(opt_state)  show_field(u)"},{"location":"notebooks/pinn_burgers.html#physics-informed-neural-networks","title":"Physics informed neural networks\u00b6","text":"<p>This piece of code reproduces the work of Raissi, Perdikaris, and Karniadakis on Physics Infomed Neural Networks, applied to the Burgers' equation.</p> <p>$$ \\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} - \\frac{0.01}{\\pi}\\frac{\\partial^2u}{\\partial t^2} = 0 \\\\ $$</p> <p>With initial and boundary conditions $$ u(0,x) = -sin(\\pi x) \\\\ u(t,-1) = u(t,1) = 0 $$</p>"},{"location":"notebooks/quickstart.html","title":"Quickstart","text":"<p>This notebook is designed to showcase the primary features of <code>jaxdf</code>. Throughout the notebook, we will utilize several libraries from the <code>jax</code> ecosystem, including <code>optax</code> and <code>diffrax</code>. Although these libraries are not necessary for using <code>jaxdf</code>, our goal is to demonstrate <code>jaxdf</code>'s compatibility with generic <code>jax</code>-based libraries.</p> <p>Let's begin by downloading an image that we will use in this notebook.</p> In\u00a0[2]: Copied! <pre>!wget -O test_img.jpg https://upload.wikimedia.org/wikipedia/commons/a/a0/Black_and_White_1_-_Augusto_De_Luca_photographer.jpg  &gt;/dev/null 2&gt;&amp;1\n</pre> !wget -O test_img.jpg https://upload.wikimedia.org/wikipedia/commons/a/a0/Black_and_White_1_-_Augusto_De_Luca_photographer.jpg  &gt;/dev/null 2&gt;&amp;1 In\u00a0[3]: Copied! <pre>!pip install opencv-python  # Run if opencv is not installed in your system\n!pip install matplotlib     # Run if matplotlib is not installed in your system\n</pre> !pip install opencv-python  # Run if opencv is not installed in your system !pip install matplotlib     # Run if matplotlib is not installed in your system <pre>Requirement already satisfied: opencv-python in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (4.8.1.78)\nRequirement already satisfied: numpy&gt;=1.21.2 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from opencv-python) (1.25.0)\nRequirement already satisfied: matplotlib in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (3.8.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (4.45.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&lt;2,&gt;=1.21 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (1.25.0)\nRequirement already satisfied: packaging&gt;=20.0 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (23.1)\nRequirement already satisfied: pillow&gt;=8 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\n</pre> In\u00a0[4]: Copied! <pre>from jax import config\nconfig.update(\"jax_enable_x64\", True)\n</pre> from jax import config config.update(\"jax_enable_x64\", True) In\u00a0[5]: Copied! <pre>import cv2\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimg = cv2.imread(\"test_img.jpg\")\nimg = cv2.resize(img, (256, 256), interpolation=cv2.INTER_AREA)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.\nplt.imshow(img, cmap=\"gray\")\nplt.colorbar()\nplt.show()\n</pre> import cv2 from matplotlib import pyplot as plt import numpy as np  img = cv2.imread(\"test_img.jpg\") img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_AREA) img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255. plt.imshow(img, cmap=\"gray\") plt.colorbar() plt.show() In\u00a0[6]: Copied! <pre>from jaxdf.geometry import Domain\n\n# Setting the domain.\ndomain = Domain(N=img.shape, dx=(1., 1.))\n</pre> from jaxdf.geometry import Domain  # Setting the domain. domain = Domain(N=img.shape, dx=(1., 1.)) In\u00a0[7]: Copied! <pre>from jax import numpy as jnp\nimport numpy as np\nfrom jaxdf.discretization import FiniteDifferences, OnGrid\n\n# We define the grid values from the image, making sure they are a jax.numpy array of floats\ngrid_values = jnp.asarray(img, dtype=np.float32)         \n\n# Then the field is defined by the combination of grid values and domain\nu = FiniteDifferences.from_grid(grid_values, domain)\n</pre> from jax import numpy as jnp import numpy as np from jaxdf.discretization import FiniteDifferences, OnGrid  # We define the grid values from the image, making sure they are a jax.numpy array of floats grid_values = jnp.asarray(img, dtype=np.float32)           # Then the field is defined by the combination of grid values and domain u = FiniteDifferences.from_grid(grid_values, domain) In\u00a0[8]: Copied! <pre>from matplotlib import pyplot as plt\n\n# We can get the field on the domain grid using the .on_grid method\nplt.imshow(u.on_grid, cmap=\"gray\")\n</pre> from matplotlib import pyplot as plt  # We can get the field on the domain grid using the .on_grid method plt.imshow(u.on_grid, cmap=\"gray\") Out[8]: <pre>&lt;matplotlib.image.AxesImage at 0x7f60181c9510&gt;</pre> <p>Now, let's define the right-hand side of the heat equation. As the only operator required is the Laplacian, we can employ <code>operators.laplacian</code> to calculate it.</p> In\u00a0[9]: Copied! <pre>from jaxdf.operators.differential import laplacian\nfrom jax import jit\n\n# Make RHS operator\n@jit   # We can jit the entire function, `Field`s from jaxdf are compatible with jax\ndef heat_rhs(u):\n    return laplacian(u)\n\n# Apply the rhs of the operator\nz = heat_rhs(u)\n</pre> from jaxdf.operators.differential import laplacian from jax import jit  # Make RHS operator @jit   # We can jit the entire function, `Field`s from jaxdf are compatible with jax def heat_rhs(u):     return laplacian(u)  # Apply the rhs of the operator z = heat_rhs(u) <pre>2023-11-24 16:05:46.095035: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng1{k2=2,k3=0} for conv (f64[1,1,256,256]{3,2,1,0}, u8[0]{0}) custom-call(f64[1,1,264,264]{3,2,1,0}, f64[1,1,9,9]{3,2,1,0}), window={size=9x9}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n2023-11-24 16:05:48.972354: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 3.877431494s\nTrying algorithm eng1{k2=2,k3=0} for conv (f64[1,1,256,256]{3,2,1,0}, u8[0]{0}) custom-call(f64[1,1,264,264]{3,2,1,0}, f64[1,1,9,9]{3,2,1,0}), window={size=9x9}, dim_labels=bf01_oi01-&gt;bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n</pre> In\u00a0[10]: Copied! <pre># Look at the output of the laplacian\nplt.figure(figsize=(8,6))\nplt.imshow(z.on_grid, cmap='RdBu'); plt.colorbar()\n</pre> # Look at the output of the laplacian plt.figure(figsize=(8,6)) plt.imshow(z.on_grid, cmap='RdBu'); plt.colorbar() Out[10]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f5f10256590&gt;</pre> In\u00a0[11]: Copied! <pre>!pip install diffrax # Install this for numerical integration in jax\n</pre> !pip install diffrax # Install this for numerical integration in jax <pre>Requirement already satisfied: diffrax in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (0.4.1)\nRequirement already satisfied: jax&gt;=0.4.13 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from diffrax) (0.4.20)\nRequirement already satisfied: equinox&gt;=0.10.11 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from diffrax) (0.11.2)\nRequirement already satisfied: jaxtyping&gt;=0.2.20 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from equinox&gt;=0.10.11-&gt;diffrax) (0.2.23)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from equinox&gt;=0.10.11-&gt;diffrax) (4.6.3)\nRequirement already satisfied: ml-dtypes&gt;=0.2.0 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from jax&gt;=0.4.13-&gt;diffrax) (0.2.0)\nRequirement already satisfied: numpy&gt;=1.22 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from jax&gt;=0.4.13-&gt;diffrax) (1.25.0)\nRequirement already satisfied: opt-einsum in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from jax&gt;=0.4.13-&gt;diffrax) (3.3.0)\nRequirement already satisfied: scipy&gt;=1.9 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from jax&gt;=0.4.13-&gt;diffrax) (1.9.3)\nRequirement already satisfied: typeguard&lt;3,&gt;=2.13.3 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from jaxtyping&gt;=0.2.20-&gt;equinox&gt;=0.10.11-&gt;diffrax) (2.13.3)\n</pre> In\u00a0[12]: Copied! <pre># Integrate with diffrax\nimport diffrax as dfx\n\ndef pde_field(t, u, args):\n  return laplacian(u)\n\nterm = dfx.ODETerm(pde_field)\nt0 = 0\nt_final = 20\ndt = 0.1\nsaveat = dfx.SaveAt(ts=jnp.linspace(t0, t_final, 10))\nsolver = dfx.Tsit5()\n\nsol = dfx.diffeqsolve(term, solver, t0, t_final, dt, u, saveat=saveat)\n</pre> # Integrate with diffrax import diffrax as dfx  def pde_field(t, u, args):   return laplacian(u)  term = dfx.ODETerm(pde_field) t0 = 0 t_final = 20 dt = 0.1 saveat = dfx.SaveAt(ts=jnp.linspace(t0, t_final, 10)) solver = dfx.Tsit5()  sol = dfx.diffeqsolve(term, solver, t0, t_final, dt, u, saveat=saveat) In\u00a0[13]: Copied! <pre>snapshots = sol.ys\n\n# Plot the solutions on a grid\nfig, ax = plt.subplots(2, 5, figsize=(10,4))\n# Flatten the axes\nax = ax.flatten()\nfor i in range(10):\n  ax[i].imshow(snapshots[i].on_grid, vmin=0, vmax=1.0, cmap=\"gray\")\n  ax[i].set_title(f\"t={sol.ts[i]:.2f}\")\n  ax[i].set_xticks([])\n  ax[i].set_yticks([])\n</pre> snapshots = sol.ys  # Plot the solutions on a grid fig, ax = plt.subplots(2, 5, figsize=(10,4)) # Flatten the axes ax = ax.flatten() for i in range(10):   ax[i].imshow(snapshots[i].on_grid, vmin=0, vmax=1.0, cmap=\"gray\")   ax[i].set_title(f\"t={sol.ts[i]:.2f}\")   ax[i].set_xticks([])   ax[i].set_yticks([]) In\u00a0[14]: Copied! <pre>from jaxdf.operators.differential import laplacian, gradient, diag_jacobian\nfrom jaxdf.operators.functions import compose, sum_over_dims\n\n# What if we want to use anisotropic diffusion?\ndef divergence(u, stagger): # Defining the divergence operator\n  return sum_over_dims(diag_jacobian(u, stagger=stagger))\n\ndef conductivity_kernel(u): # Defining the diffusion kernel\n  kernel = lambda x: 1/(1 + (x/0.03)**2)\n  return compose(u)(kernel)\n\ndef norm(u):\n  z = sum_over_dims(u**2)\n  return compose(z)(jnp.sqrt)\n\n@jit\ndef anisotropic_diffusion(t, u, args):\n  grad_u = gradient(u, stagger=[0.5])\n  mod_gradient = norm(grad_u)\n  c = conductivity_kernel(mod_gradient)\n  return divergence(c * grad_u, stagger=[-0.5])\n\n# Plot the effect of the kernel\nz = anisotropic_diffusion(0, u, None)\nplt.imshow(z.on_grid, cmap=\"RdBu\")\nplt.colorbar()\nplt.savefig(\"anisotropic_kernel.png\")\nplt.close()\n\nterm = dfx.ODETerm(anisotropic_diffusion)\nsol = dfx.diffeqsolve(term, solver, t0, t_final, dt, u, saveat=saveat)\n</pre> from jaxdf.operators.differential import laplacian, gradient, diag_jacobian from jaxdf.operators.functions import compose, sum_over_dims  # What if we want to use anisotropic diffusion? def divergence(u, stagger): # Defining the divergence operator   return sum_over_dims(diag_jacobian(u, stagger=stagger))  def conductivity_kernel(u): # Defining the diffusion kernel   kernel = lambda x: 1/(1 + (x/0.03)**2)   return compose(u)(kernel)  def norm(u):   z = sum_over_dims(u**2)   return compose(z)(jnp.sqrt)  @jit def anisotropic_diffusion(t, u, args):   grad_u = gradient(u, stagger=[0.5])   mod_gradient = norm(grad_u)   c = conductivity_kernel(mod_gradient)   return divergence(c * grad_u, stagger=[-0.5])  # Plot the effect of the kernel z = anisotropic_diffusion(0, u, None) plt.imshow(z.on_grid, cmap=\"RdBu\") plt.colorbar() plt.savefig(\"anisotropic_kernel.png\") plt.close()  term = dfx.ODETerm(anisotropic_diffusion) sol = dfx.diffeqsolve(term, solver, t0, t_final, dt, u, saveat=saveat) In\u00a0[15]: Copied! <pre>snapshots = sol.ys\n\n# Plot the solutions on a grid\nfig, ax = plt.subplots(2, 5, figsize=(10,4))\n# Flatten the axes\nax = ax.flatten()\nfor i in range(10):\n  ax[i].imshow(snapshots[i].on_grid, vmin=0, vmax=1.0, cmap=\"gray\")\n  ax[i].set_title(f\"t={sol.ts[i]:.2f}\")\n  ax[i].set_xticks([])\n  ax[i].set_yticks([])\n</pre> snapshots = sol.ys  # Plot the solutions on a grid fig, ax = plt.subplots(2, 5, figsize=(10,4)) # Flatten the axes ax = ax.flatten() for i in range(10):   ax[i].imshow(snapshots[i].on_grid, vmin=0, vmax=1.0, cmap=\"gray\")   ax[i].set_title(f\"t={sol.ts[i]:.2f}\")   ax[i].set_xticks([])   ax[i].set_yticks([]) <p>By constructing the anisotropic diffusion operator using <code>jaxdf</code>, we've highlighted several features of the library:</p> <ul> <li>Composition: Ability to define complex differential operators, such as the divergence, gradient, and custom conductivity kernel, using other operators.</li> <li>Performance: Using <code>@jit</code> to just-in-time compile our functions, ensuring optimal performance during execution.</li> </ul> In\u00a0[16]: Copied! <pre>from jaxdf.discretization import FourierSeries\n\nu_f = FourierSeries.from_grid(u.on_grid, domain)\n\n# We can reuse the previous code!\nsol = dfx.diffeqsolve(term, solver, t0, t_final, dt, u_f, saveat=saveat)\nsnapshots_fourier = sol.ys\n</pre> from jaxdf.discretization import FourierSeries  u_f = FourierSeries.from_grid(u.on_grid, domain)  # We can reuse the previous code! sol = dfx.diffeqsolve(term, solver, t0, t_final, dt, u_f, saveat=saveat) snapshots_fourier = sol.ys In\u00a0[17]: Copied! <pre># Plot the solutions on a grid\nfig, ax = plt.subplots(1, 3, figsize=(10, 3))\nax = ax.flatten()\nax[0].imshow(snapshots[-1].on_grid, vmin=0, vmax=0.8, cmap=\"gray\")\nax[0].set_title(f\"FiniteDifferences\")\nax[1].imshow(snapshots_fourier[-1].on_grid, vmin=0, vmax=0.8, cmap=\"gray\")\nax[1].set_title(f\"FourierSeries\")\ndifference = np.abs(snapshots_fourier[-1].on_grid - snapshots[-1].on_grid)\nax[2].imshow(difference, vmin=0, vmax=0.1, cmap=\"inferno\")\nax[2].set_title(f\"Abs. difference\")\n\nfor i in range(3):\n    ax[i].set_xticks([])\n    ax[i].set_yticks([])\n</pre> # Plot the solutions on a grid fig, ax = plt.subplots(1, 3, figsize=(10, 3)) ax = ax.flatten() ax[0].imshow(snapshots[-1].on_grid, vmin=0, vmax=0.8, cmap=\"gray\") ax[0].set_title(f\"FiniteDifferences\") ax[1].imshow(snapshots_fourier[-1].on_grid, vmin=0, vmax=0.8, cmap=\"gray\") ax[1].set_title(f\"FourierSeries\") difference = np.abs(snapshots_fourier[-1].on_grid - snapshots[-1].on_grid) ax[2].imshow(difference, vmin=0, vmax=0.1, cmap=\"inferno\") ax[2].set_title(f\"Abs. difference\")  for i in range(3):     ax[i].set_xticks([])     ax[i].set_yticks([]) In\u00a0[18]: Copied! <pre>from jax import value_and_grad\n\n@value_and_grad\ndef loss_fn(u: FourierSeries):\n    y = compose(2*jnp.pi*u)(jnp.sin)\n    z = laplacian(y)\n    return jnp.mean(z.on_grid**2)\n\nlossval, z = loss_fn(u_f)\n\n# The gradient is once again a Field\nplt.imshow(z.on_grid, cmap=\"RdBu\" , vmin=-0.01, vmax=0.01)\nplt.colorbar()\nprint(f\"Loss: {lossval}\")\n</pre> from jax import value_and_grad  @value_and_grad def loss_fn(u: FourierSeries):     y = compose(2*jnp.pi*u)(jnp.sin)     z = laplacian(y)     return jnp.mean(z.on_grid**2)  lossval, z = loss_fn(u_f)  # The gradient is once again a Field plt.imshow(z.on_grid, cmap=\"RdBu\" , vmin=-0.01, vmax=0.01) plt.colorbar() print(f\"Loss: {lossval}\") <pre>Loss: 1.5509734922469067\n</pre> In\u00a0[19]: Copied! <pre># Get the operator parameters for a FiniteDifferences field\n# (remember that type(u) = FiniteDifferences)\nfd_stencil = laplacian.default_params(u)     \n\nplt.imshow(fd_stencil, vmin=-1, vmax=1, cmap=\"RdBu\")\nplt.colorbar()\nplt.axis('off')\nplt.title(\"Laplacian stencil for FiniteDifferences fields\")\n</pre> # Get the operator parameters for a FiniteDifferences field # (remember that type(u) = FiniteDifferences) fd_stencil = laplacian.default_params(u)       plt.imshow(fd_stencil, vmin=-1, vmax=1, cmap=\"RdBu\") plt.colorbar() plt.axis('off') plt.title(\"Laplacian stencil for FiniteDifferences fields\") Out[19]: <pre>Text(0.5, 1.0, 'Laplacian stencil for FiniteDifferences fields')</pre> In\u00a0[20]: Copied! <pre># Get the operator parameters for a FourierSeries field\n# (remember that type(u) = FourierSeries)\nf_params = laplacian.default_params(u_f)\nf_params.keys()\n</pre> # Get the operator parameters for a FourierSeries field # (remember that type(u) = FourierSeries) f_params = laplacian.default_params(u_f) f_params.keys() Out[20]: <pre>dict_keys(['k_vec'])</pre> <p>By design, these parameters are statically compiled into the XLA function. However, <code>jaxdf</code> offers flexibility for scenarios such as when:</p> <ul> <li>Parameters have been manually adjusted.</li> <li>Parameters are sizeable, making static compilation inefficient.</li> <li>Different operators share the same parameters, as seen in Fourier methods. In such cases, users might desire to use a consistent variable across all.</li> </ul> <p>To accommodate these scenarios, every <code>operator</code> in <code>jaxdf</code> has a <code>params</code> keyword. This reserved keyword allows users to override default parameters, giving them precise control over the computation.</p> In\u00a0[21]: Copied! <pre># Get the default parameters\nstencil = laplacian.default_params(u)\n\n# Change the middle element\nstencil[5,5] = -10.0\n\n# Call the laplacian with the new parameters\nz = laplacian(u, params=stencil)\n\n# Check the result\nplt.imshow(z.on_grid)\nplt.colorbar()\n</pre> # Get the default parameters stencil = laplacian.default_params(u)  # Change the middle element stencil[5,5] = -10.0  # Call the laplacian with the new parameters z = laplacian(u, params=stencil)  # Check the result plt.imshow(z.on_grid) plt.colorbar() Out[21]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f604fade590&gt;</pre> <p><code>jaxdf</code> doesn't just stop at providing access to parameters. Users can also apply functional transformations directly to these parameters too!</p> <p>To illustrate this, let's embark on an example. We'll optimize the stencil of the <code>laplacian</code> operator for a <code>FiniteDifferences</code> field to align its outcome more closely with the <code>FourierSeries</code> version.</p> In\u00a0[22]: Copied! <pre>stencil = laplacian.default_params(u)\n\n@value_and_grad\ndef loss_fn(stencil, u: FiniteDifferences, v: FourierSeries):\n    z_fd = laplacian(u, params=stencil)  # Note the explicit parameters being passed\n    z_fs = laplacian(v)\n\n    mse = jnp.sum(((z_fd - z_fs)**2).on_grid)\n    return mse\n\nlossval, stencil_grad = loss_fn(stencil, u, u_f)\n</pre> stencil = laplacian.default_params(u)  @value_and_grad def loss_fn(stencil, u: FiniteDifferences, v: FourierSeries):     z_fd = laplacian(u, params=stencil)  # Note the explicit parameters being passed     z_fs = laplacian(v)      mse = jnp.sum(((z_fd - z_fs)**2).on_grid)     return mse  lossval, stencil_grad = loss_fn(stencil, u, u_f) In\u00a0[23]: Copied! <pre>!pip install optax    # Install optax for optimization in jax\n</pre> !pip install optax    # Install optax for optimization in jax <pre>Requirement already satisfied: optax in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (0.1.7)\nRequirement already satisfied: absl-py&gt;=0.7.1 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from optax) (2.0.0)\nRequirement already satisfied: chex&gt;=0.1.5 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from optax) (0.1.84)\nRequirement already satisfied: jax&gt;=0.1.55 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from optax) (0.4.20)\nRequirement already satisfied: jaxlib&gt;=0.1.37 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from optax) (0.4.20+cuda12.cudnn89)\nRequirement already satisfied: numpy&gt;=1.18.0 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from optax) (1.25.0)\nRequirement already satisfied: typing-extensions&gt;=4.2.0 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from chex&gt;=0.1.5-&gt;optax) (4.6.3)\nRequirement already satisfied: toolz&gt;=0.9.0 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from chex&gt;=0.1.5-&gt;optax) (0.12.0)\nRequirement already satisfied: ml-dtypes&gt;=0.2.0 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from jax&gt;=0.1.55-&gt;optax) (0.2.0)\nRequirement already satisfied: opt-einsum in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from jax&gt;=0.1.55-&gt;optax) (3.3.0)\nRequirement already satisfied: scipy&gt;=1.9 in /home/antonio/anaconda3/envs/jaxdf/lib/python3.11/site-packages (from jax&gt;=0.1.55-&gt;optax) (1.9.3)\n</pre> In\u00a0[24]: Copied! <pre>import optax\n\noptimizer = optax.adam(0.02)\nopt_state = optimizer.init(stencil)\nnew_stencil = stencil\n\n@jit\ndef step(stencil, opt_state, u, v):\n    lossval, stencil_grad = loss_fn(stencil, u, u_f)\n    \n    updates, opt_state = optimizer.update(stencil_grad, opt_state)\n    stencil = optax.apply_updates(stencil, updates)\n    return lossval, stencil, opt_state\n\nfor i in range(4001):\n    lossval, new_stencil, opt_state = step(new_stencil, opt_state, u, u_f) \n    if i % 500 == 0:\n        print(f\"Step: {i} - Loss {lossval}\")\n</pre> import optax  optimizer = optax.adam(0.02) opt_state = optimizer.init(stencil) new_stencil = stencil  @jit def step(stencil, opt_state, u, v):     lossval, stencil_grad = loss_fn(stencil, u, u_f)          updates, opt_state = optimizer.update(stencil_grad, opt_state)     stencil = optax.apply_updates(stencil, updates)     return lossval, stencil, opt_state  for i in range(4001):     lossval, new_stencil, opt_state = step(new_stencil, opt_state, u, u_f)      if i % 500 == 0:         print(f\"Step: {i} - Loss {lossval}\") <pre>Step: 0 - Loss 142.6593552085341\nStep: 500 - Loss 74.48526520856781\nStep: 1000 - Loss 39.88113557681022\nStep: 1500 - Loss 20.86625953473459\nStep: 2000 - Loss 13.724514375728456\nStep: 2500 - Loss 10.244072447332503\nStep: 3000 - Loss 8.509387549827993\nStep: 3500 - Loss 7.6116108961437625\nStep: 4000 - Loss 8.342067758718336\n</pre> In\u00a0[25]: Copied! <pre># Check the performances\nz_FD  = laplacian(u)\nz_FS  = laplacian(u_f)\nz_opt = laplacian(u, params = new_stencil)\n\nfd_diff = jnp.abs((z_FD - z_FS).on_grid)\nopt_diff = jnp.abs((z_opt - z_FS).on_grid)\n\nfig, ax = plt.subplots(2, 3, figsize=(10, 6))\nax = ax.flatten()\nax[0].imshow(z_FS.on_grid, vmin=-1, vmax=1, cmap=\"RdBu\")\nax[0].set_title(f\"FourierSeries\")\nax[1].imshow(z_FD.on_grid, vmin=-1, vmax=1, cmap=\"RdBu\")\nax[1].set_title(f\"FiniteDifferences\")\nax[2].imshow(fd_diff, vmin=0, vmax=0.3, cmap=\"inferno\")\nax[2].set_title(f\"FiniteDifferences Error\")\nax[3].imshow(new_stencil, vmin=-1, vmax=1, cmap=\"RdBu\")\nax[3].set_title(f\"Optimized Stencil\")\nax[4].imshow(z_opt.on_grid, vmin=-1, vmax=1, cmap=\"RdBu\")\nax[4].set_title(f\"Optimized\")\nax[5].imshow(opt_diff, vmin=0, vmax=0.3, cmap=\"inferno\")\nax[5].set_title(f\"Optimized Error\")\n\nfor i in range(6):\n    ax[i].set_xticks([])\n    ax[i].set_yticks([])\n</pre> # Check the performances z_FD  = laplacian(u) z_FS  = laplacian(u_f) z_opt = laplacian(u, params = new_stencil)  fd_diff = jnp.abs((z_FD - z_FS).on_grid) opt_diff = jnp.abs((z_opt - z_FS).on_grid)  fig, ax = plt.subplots(2, 3, figsize=(10, 6)) ax = ax.flatten() ax[0].imshow(z_FS.on_grid, vmin=-1, vmax=1, cmap=\"RdBu\") ax[0].set_title(f\"FourierSeries\") ax[1].imshow(z_FD.on_grid, vmin=-1, vmax=1, cmap=\"RdBu\") ax[1].set_title(f\"FiniteDifferences\") ax[2].imshow(fd_diff, vmin=0, vmax=0.3, cmap=\"inferno\") ax[2].set_title(f\"FiniteDifferences Error\") ax[3].imshow(new_stencil, vmin=-1, vmax=1, cmap=\"RdBu\") ax[3].set_title(f\"Optimized Stencil\") ax[4].imshow(z_opt.on_grid, vmin=-1, vmax=1, cmap=\"RdBu\") ax[4].set_title(f\"Optimized\") ax[5].imshow(opt_diff, vmin=0, vmax=0.3, cmap=\"inferno\") ax[5].set_title(f\"Optimized Error\")  for i in range(6):     ax[i].set_xticks([])     ax[i].set_yticks([]) In\u00a0[26]: Copied! <pre>from jaxdf.operators.functions import compose\nfrom jaxdf import Field\nfrom jax.nn import relu\nfrom jax import random, lax\n\nseed = random.PRNGKey(42)\ncnn_kernel = random.normal(seed, (1,1,3,3))\n\ndef silly_cnn(x: jnp.ndarray, kernel: jnp.ndarray):\n    x = jnp.moveaxis(x, -1, 0)\n    x = jnp.expand_dims(x,0)\n    out_conv = lax.conv(x, cnn_kernel, (1,1), padding='same')\n    out_conv = relu(out_conv)[0]\n    out = jnp.moveaxis(out_conv, 0, -1)\n    return out\n\n@jit\ndef f(u: Field):\n    L = laplacian(u)\n    \n    field_on_grid = L.on_grid # Represent the field as a standard jnp array\n    new_grid_values = silly_cnn(field_on_grid, cnn_kernel)  # Manipulate its values with a neural network\n    p = FiniteDifferences.from_grid(new_grid_values, u.domain) # Generate a new FiniteDifferences field\n\n    # Apply jaxdf operators again\n    p = compose(p)(jnp.sin)\n    return 0.01*p\n</pre> from jaxdf.operators.functions import compose from jaxdf import Field from jax.nn import relu from jax import random, lax  seed = random.PRNGKey(42) cnn_kernel = random.normal(seed, (1,1,3,3))  def silly_cnn(x: jnp.ndarray, kernel: jnp.ndarray):     x = jnp.moveaxis(x, -1, 0)     x = jnp.expand_dims(x,0)     out_conv = lax.conv(x, cnn_kernel, (1,1), padding='same')     out_conv = relu(out_conv)[0]     out = jnp.moveaxis(out_conv, 0, -1)     return out  @jit def f(u: Field):     L = laplacian(u)          field_on_grid = L.on_grid # Represent the field as a standard jnp array     new_grid_values = silly_cnn(field_on_grid, cnn_kernel)  # Manipulate its values with a neural network     p = FiniteDifferences.from_grid(new_grid_values, u.domain) # Generate a new FiniteDifferences field      # Apply jaxdf operators again     p = compose(p)(jnp.sin)     return 0.01*p In\u00a0[27]: Copied! <pre>z = f(u)\nprint(z)\n</pre> z = f(u) print(z) <pre>FiniteDifferences(\n  params=f64[256,256,1],\n  domain=Domain(N=(256, 256), dx=(1.0, 1.0)),\n  accuracy=8\n)\n</pre>"},{"location":"notebooks/quickstart.html#quickstart","title":"Quickstart\u00b6","text":""},{"location":"notebooks/quickstart.html#heat-equation","title":"Heat Equation\u00b6","text":"<p>For illustration, we'll simulate the heat equation.</p> <p>$$ \\frac{\\partial}{\\partial t}u = \\nabla^2 u $$</p> <p>NOTE: We don't enforce specific boundary conditions here as they are not yet implemented. At present, boundary conditions are implicitly defined by padding for convolutive operators in Finite Differences and Fourier as periodic BC. While this isn't ideal for properly integrating the heat equation, it works whenever we use some form of absorbing layer at the boundary, which is often the case in acoustics.</p> <p>Obviously, a more suitable handling of generic boundary conditions would be a valuable addition to the package. Contributions are welcome \ud83d\ude0a</p> <p></p> <p>We first define the domain where fields live. Subsequently, we specify the discretization family that will represent the inputs to the operator. In this case, we use Finite Differences, as given by the <code>FiniteDifferences</code> discretization.</p>"},{"location":"notebooks/quickstart.html#integration-using-diffrax","title":"Integration using <code>diffrax</code>\u00b6","text":"<p>In this section, we will integrate our heat equation using the <code>diffrax</code> library. <code>diffrax</code> offers a suite of differential equation solvers, making it ideal for our needs.</p> <ol> <li><p>Setting up the PDE: First, we define our PDE field, which in this case is represented by the Laplacian of ( u ).</p> </li> <li><p>Setting the Solver and Time Steps:</p> <ul> <li>We choose <code>Tsit5</code> as our solver</li> <li>We set our initial time <code>t0</code> to 0 and the final time <code>t_final</code> to 20.</li> <li>The time step <code>dt</code> is set to 0.1.</li> <li>Additionally, with the <code>SaveAt</code> feature, we decide to save the results at specific, evenly spaced time points.</li> </ul> </li> <li><p>Solving the PDE: Using <code>dfx.diffeqsolve</code>, we pass in our term, the chosen solver, time configurations, and the initial field <code>u</code> to obtain the solution.</p> </li> </ol> <p>Let's dive into the code:</p>"},{"location":"notebooks/quickstart.html#anisotropic-diffusion-example","title":"Anisotropic Diffusion example\u00b6","text":"<p>Anisotropic diffusion is a more sophisticated form of heat diffusion where the diffusion rate can vary in different directions and magnitudes, often dependent on the underlying structure of the data. This is particularly useful in applications like image processing, where it can help preserve edges while diffusing noise.</p> <p>Let's see how we can leverage <code>jaxdf</code> to define and solve an anisotropic diffusion problem:</p> <p>The anisotropic diffusion equation can be represented as:</p> <p>$$ \\frac{\\partial u}{\\partial t} = \\nabla \\cdot (c(u) \\nabla u) $$</p> <p>Where:</p> <ul> <li>$u$ is the field of interest.</li> <li>$c(u)$ is the diffusion conductivity that varies depending on the gradient magnitude of ( u ).</li> </ul> <p>To translate this to code:</p> <ul> <li><p>Divergence Operator: We start by defining the divergence of a vector field, <code>divergence(u, stagger)</code>. This computes the rate at which density exits at each point, essential for understanding how diffusion spreads. Note that we are using staggered gradients, which are a feature of <code>OnGrid</code> fields.</p> </li> <li><p>Diffusion Conductivity: <code>conductivity_kernel(u)</code> computes the conductivity based on the magnitude of the gradient of $u$. The kernel ensures that areas with high gradients (like edges) have lower conductivity, preserving features.</p> </li> <li><p>Gradient Magnitude: <code>norm(u)</code> computes the magnitude of the gradient, which is essential to determine the diffusion rate.</p> </li> <li><p>Complete Anisotropic Diffusion: <code>anisotropic_diffusion(t, u, args)</code> combines the above functions to compute the divergence of the conductivity-weighted gradient of $u$, yielding the right-hand side of our PDE.</p> </li> </ul>"},{"location":"notebooks/quickstart.html#customizing-the-discretization","title":"Customizing the Discretization\u00b6","text":"<p>Exploring different discretizations is a powerful feature in <code>jaxdf</code>. For instance, if we wish to go from <code>FiniteDifferences</code> to a <code>FourierSeries</code> discretization, the transition is seamless with the following steps:</p> <ol> <li>Define a New Field: Initialize your field using the desired FourierSeries discretization.</li> <li>Invoke the Operator: Simply call the previously defined operator on the newly discretized field.</li> </ol> <p>This flexibility ensures that researchers and developers can effortlessly experiment with various discretizations, often without restructuring their primary operators.</p>"},{"location":"notebooks/quickstart.html#automatic-differentiation-with-jaxdf","title":"Automatic Differentiation with <code>jaxdf</code>\u00b6","text":"<p>One of the significant strengths of the <code>jaxdf</code> framework is its seamless integration with <code>jax</code>'s automatic differentiation capabilities. By leveraging <code>jax</code>'s native transformations, users can efficiently compute gradients with respect to the arguments of operators.</p>"},{"location":"notebooks/quickstart.html#handling-operator-parameters","title":"Handling Operator Parameters\u00b6","text":"<p>Operators in computational methods often come with associated parameters that play a critical role in their computation. In <code>jaxdf</code>, these parameters can be discretization-dependent, adapting based on the specific method in use.</p> <p>For instance, the <code>laplacian</code> operator:</p> <ul> <li>In the context of <code>FiniteDifferences</code>, it's implemented using a stencil.</li> <li>For <code>FourierSeries</code>, it relies on transformations of the <code>k-axis</code> (spatial frequency axis).</li> </ul> <p>The <code>.default_params</code> method provides insight into these parameters. When invoked with the same arguments as the operator, it returns the default parameters utilized.</p>"},{"location":"notebooks/quickstart.html#more-complex-operators","title":"More complex operators\u00b6","text":"<p>We can of course construct more complex operators than the heat equation, using composition. Beyond the core functionality of operators, <code>jaxdf</code> grants direct access to the numerical parameters of the fields. This feature is invaluable for scenarios requiring fine-tuned control or experimentation with parameters.</p>"},{"location":"notebooks/simulate_helmholtz_equation.html","title":"Optimize acoustic simulations","text":"In\u00a0[1]: Copied! <pre>from matplotlib import pyplot as plt\n\nfrom jaxdf.geometry import Domain\nfrom jax import numpy as jnp\nimport jax\n</pre> from matplotlib import pyplot as plt  from jaxdf.geometry import Domain from jax import numpy as jnp import jax <p>The equation we want to simulate is</p> <p>$$ \\left(\\nabla^2 + \\frac{\\omega^2}{c^2} \\right)u = - i\\omega S_M $$</p> <p>Furthermore, to enforce the Sommerfield radiation conditions, the components of the spatial differential operators are modified as [1]:</p> <p>$$ \\partial_{x_j} = \\frac{\\partial_{x_j}}{\\gamma_j} $$</p> <p>where</p> <p>$$ \\gamma_j = 1 + \\frac{i\\sigma_j(x)}{k_0}, \\qquad \\sigma_j(x) = \\begin{cases} \\frac{\\sigma^*\\Big| |x_j| - \\Delta_{PML}\\Big|^\\alpha}{L/2-\\Delta_{PML}}, &amp; \\|x\\| &gt; \\Delta_{PML}\\\\ 0, &amp; \\text{otherwise} \\\\ \\end{cases}$$</p> In\u00a0[2]: Copied! <pre>domain = Domain(N=(256, 256), dx=(1., 1.))\n</pre> domain = Domain(N=(256, 256), dx=(1., 1.)) In\u00a0[3]: Copied! <pre>from jaxdf.discretization import Continuous\n\nx = Continuous(None, domain, lambda p, x: x)\n</pre> from jaxdf.discretization import Continuous  x = Continuous(None, domain, lambda p, x: x) In\u00a0[5]: Copied! <pre>grid = x.on_grid\n\n# Plot the field\nfig, ax = plt.subplots(1,2,figsize=(12,5))\n\nax[0].imshow(grid[...,0], cmap=\"inferno\")\nax[0].set_title(\"$x$-component of the field x\")\n\nax[1].imshow(grid[...,1], cmap=\"inferno\")\nax[1].set_title(\"$y$-component of the field x\")\n</pre> grid = x.on_grid  # Plot the field fig, ax = plt.subplots(1,2,figsize=(12,5))  ax[0].imshow(grid[...,0], cmap=\"inferno\") ax[0].set_title(\"$x$-component of the field x\")  ax[1].imshow(grid[...,1], cmap=\"inferno\") ax[1].set_title(\"$y$-component of the field x\") Out[5]: <pre>Text(0.5, 1.0, '$y$-component of the field x')</pre> <p>To construct $\\sigma$, let's start from its JAX-numpy implementation</p> In\u00a0[6]: Copied! <pre>def sigma(x):\n    alpha = 2.\n    sigma_star = 1.\n    delta_pml = 110.\n    L_half = 128.\n    \n    abs_x = jnp.abs(x)\n    in_pml_amplitude = (jnp.abs(abs_x-delta_pml)/(L_half - delta_pml))**alpha\n    return jnp.where(abs_x &gt; delta_pml, sigma_star*in_pml_amplitude, 0.)\n</pre> def sigma(x):     alpha = 2.     sigma_star = 1.     delta_pml = 110.     L_half = 128.          abs_x = jnp.abs(x)     in_pml_amplitude = (jnp.abs(abs_x-delta_pml)/(L_half - delta_pml))**alpha     return jnp.where(abs_x &gt; delta_pml, sigma_star*in_pml_amplitude, 0.) <p>We can apply this function over the field $x$ using the <code>compose</code> operator</p> In\u00a0[7]: Copied! <pre>from jaxdf.operators import compose\n\ny = compose(x)(sigma)\nprint('y: ', y)\n</pre> from jaxdf.operators import compose  y = compose(x)(sigma) print('y: ', y) <pre>y:  Field Continuous\n</pre> In\u00a0[8]: Copied! <pre>grid = y.on_grid\n\n# Plot the field\nfig, ax = plt.subplots(1,2,figsize=(12,5))\n\nax[0].imshow(grid[...,0], cmap=\"inferno\")\nax[0].set_title(\"$x$-component of the field $\\sigma(x)$\")\n\nax[1].imshow(grid[...,1], cmap=\"inferno\")\nax[1].set_title(\"$y$-component of the field $\\sigma(x)$\")\n</pre> grid = y.on_grid  # Plot the field fig, ax = plt.subplots(1,2,figsize=(12,5))  ax[0].imshow(grid[...,0], cmap=\"inferno\") ax[0].set_title(\"$x$-component of the field $\\sigma(x)$\")  ax[1].imshow(grid[...,1], cmap=\"inferno\") ax[1].set_title(\"$y$-component of the field $\\sigma(x)$\") Out[8]: <pre>Text(0.5, 1.0, '$y$-component of the field $\\\\sigma(x)$')</pre> In\u00a0[9]: Copied! <pre>def gamma(x, omega=1.):\n    y = compose(x)(sigma)\n    return 1./(1. + 1j*y/omega)\n\npml = gamma(x)\n</pre> def gamma(x, omega=1.):     y = compose(x)(sigma)     return 1./(1. + 1j*y/omega)  pml = gamma(x) In\u00a0[10]: Copied! <pre>pml_on_grid = pml.on_grid\n\n# Plot the field\nfig, ax = plt.subplots(1,2,figsize=(10,4))\nax[0].imshow(pml_on_grid[...,0].real, cmap=\"inferno\")\nax[0].set_title(\"$x$-component of (Real)\")\nax[1].imshow(pml_on_grid[...,1].imag, cmap=\"inferno\")\nax[1].set_title(\"$y$-component of (Imag.)\")\nplt.show()\n</pre> pml_on_grid = pml.on_grid  # Plot the field fig, ax = plt.subplots(1,2,figsize=(10,4)) ax[0].imshow(pml_on_grid[...,0].real, cmap=\"inferno\") ax[0].set_title(\"$x$-component of (Real)\") ax[1].imshow(pml_on_grid[...,1].imag, cmap=\"inferno\") ax[1].set_title(\"$y$-component of (Imag.)\") plt.show() In\u00a0[13]: Copied! <pre>from jaxdf.operators import gradient, diag_jacobian, sum_over_dims\n\ndef mod_laplacian(u, pml):\n    # No nyquist correction, see https://math.mit.edu/~stevenj/fft-deriv.pdf\n    grad_u = gradient(u, correct_nyquist=False)\n    mod_grad_u = grad_u*pml\n    mod_diag_jacobian = diag_jacobian(mod_grad_u, correct_nyquist=False)*pml\n    return sum_over_dims(mod_diag_jacobian)\n</pre> from jaxdf.operators import gradient, diag_jacobian, sum_over_dims  def mod_laplacian(u, pml):     # No nyquist correction, see https://math.mit.edu/~stevenj/fft-deriv.pdf     grad_u = gradient(u, correct_nyquist=False)     mod_grad_u = grad_u*pml     mod_diag_jacobian = diag_jacobian(mod_grad_u, correct_nyquist=False)*pml     return sum_over_dims(mod_diag_jacobian) In\u00a0[14]: Copied! <pre>from jaxdf.discretization import FourierSeries\n\n# Defining numerical parameters\nparams = jnp.zeros((256,256,1)) + 0j\nsrc_params = params.at[128,64].set(1.0)\n\nc_fourier_params = params + 1.  \nc_fourier_params = c_fourier_params.at[20:40,20:100].set(2.)\nc_fourier_params = c_fourier_params.at[20:200,150:200].set(1.5)\n\n# Defining fields\nu = FourierSeries(params, domain)\npml = FourierSeries(pml_on_grid, domain)\nc =  FourierSeries(c_fourier_params, domain)\nsrc = FourierSeries(src_params, domain)\n</pre> from jaxdf.discretization import FourierSeries  # Defining numerical parameters params = jnp.zeros((256,256,1)) + 0j src_params = params.at[128,64].set(1.0)  c_fourier_params = params + 1.   c_fourier_params = c_fourier_params.at[20:40,20:100].set(2.) c_fourier_params = c_fourier_params.at[20:200,150:200].set(1.5)  # Defining fields u = FourierSeries(params, domain) pml = FourierSeries(pml_on_grid, domain) c =  FourierSeries(c_fourier_params, domain) src = FourierSeries(src_params, domain) In\u00a0[15]: Copied! <pre>@jax.jit\ndef helmholtz(u, c, pml, omega=1.):\n    # Get the modified laplacian\n    L = mod_laplacian(u, pml)\n    \n    # Add the wavenumber term\n    k = ((omega/c)**2)*u    \n    return L + k\n</pre> @jax.jit def helmholtz(u, c, pml, omega=1.):     # Get the modified laplacian     L = mod_laplacian(u, pml)          # Add the wavenumber term     k = ((omega/c)**2)*u         return L + k In\u00a0[16]: Copied! <pre># Wrap around a function to work with GMRES\ndef helm_func(u):\n    return helmholtz(u, c, pml)\n</pre> # Wrap around a function to work with GMRES def helm_func(u):     return helmholtz(u, c, pml) In\u00a0[17]: Copied! <pre>from jax.scipy.sparse.linalg import gmres\nfrom functools import partial\n\nsol, _ = gmres(helm_func, src, maxiter=1000)\n</pre> from jax.scipy.sparse.linalg import gmres from functools import partial  sol, _ = gmres(helm_func, src, maxiter=1000) In\u00a0[20]: Copied! <pre>sol_on_grid = sol.on_grid\n\nplt.figure(figsize=(10,8))\nplt.imshow(jnp.real(sol_on_grid), cmap=\"RdBu_r\", vmin=-.04, vmax=.04)\nplt.colorbar()\n</pre> sol_on_grid = sol.on_grid  plt.figure(figsize=(10,8)) plt.imshow(jnp.real(sol_on_grid), cmap=\"RdBu_r\", vmin=-.04, vmax=.04) plt.colorbar() Out[20]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f7e940b2fa0&gt;</pre> In\u00a0[26]: Copied! <pre>from jaxdf.operators import laplacian, compose\n\n@jax.jit\ndef total_variation(u):\n    nabla_u = laplacian(u)\n    absval = compose(nabla_u)(jnp.abs)\n    return jnp.mean(absval.on_grid)\n</pre> from jaxdf.operators import laplacian, compose  @jax.jit def total_variation(u):     nabla_u = laplacian(u)     absval = compose(nabla_u)(jnp.abs)     return jnp.mean(absval.on_grid) In\u00a0[27]: Copied! <pre>from jax import random\nseed = random.PRNGKey(32)\n\n# Those are the parameters we want to optimize\nlens_params = random.uniform(seed, (168,40))-4\n\ndef get_sos(T):\n    lens = jnp.zeros(domain.N)\n    lens = lens.at[44:212,108:148].set(jax.nn.sigmoid(T)) + 1\n    return FourierSeries(jnp.expand_dims(lens, -1), domain)\n\ndef find_field(c, x0):\n    def helm_func(u):\n        return helmholtz(u, c, pml)\n    sol, _ = gmres(helm_func, src, x0=x0, maxiter=1000, tol=1e-3, restart=10)\n    return sol\n\ndef loss(p, guess):\n    sos = get_sos(p)\n    tv_term = total_variation(sos)\n    field = find_field(sos, x0=guess)\n    target_val = field.on_grid[70,210,0]\n    return (-jnp.abs(target_val)) + tv_term, field\n</pre> from jax import random seed = random.PRNGKey(32)  # Those are the parameters we want to optimize lens_params = random.uniform(seed, (168,40))-4  def get_sos(T):     lens = jnp.zeros(domain.N)     lens = lens.at[44:212,108:148].set(jax.nn.sigmoid(T)) + 1     return FourierSeries(jnp.expand_dims(lens, -1), domain)  def find_field(c, x0):     def helm_func(u):         return helmholtz(u, c, pml)     sol, _ = gmres(helm_func, src, x0=x0, maxiter=1000, tol=1e-3, restart=10)     return sol  def loss(p, guess):     sos = get_sos(p)     tv_term = total_variation(sos)     field = find_field(sos, x0=guess)     target_val = field.on_grid[70,210,0]     return (-jnp.abs(target_val)) + tv_term, field In\u00a0[28]: Copied! <pre>from jax.example_libraries import optimizers\nfrom jax import jit\nfrom tqdm import tqdm\n\nlosshistory = []\n\ninit_fun, update_fun, get_params = optimizers.adam(.1, b1=0.9, b2=0.9)\nopt_state = init_fun(lens_params)\n\n@jit\ndef update(opt_state, field):\n    loss_and_field, gradient = jax.value_and_grad(loss, has_aux=True)(get_params(opt_state), field)\n    return loss_and_field[0], loss_and_field[1], update_fun(k, gradient, opt_state)\n\npbar = tqdm(range(100))\nfield = -src\nfor k in pbar:\n    lossval, field, opt_state = update(opt_state, field)\n    # For logging\n    pbar.set_description(\"Ampl: {:01.4f}\".format(-lossval))\n    losshistory.append(lossval)\n    \ntransmit_phase = get_params(opt_state)\n</pre> from jax.example_libraries import optimizers from jax import jit from tqdm import tqdm  losshistory = []  init_fun, update_fun, get_params = optimizers.adam(.1, b1=0.9, b2=0.9) opt_state = init_fun(lens_params)  @jit def update(opt_state, field):     loss_and_field, gradient = jax.value_and_grad(loss, has_aux=True)(get_params(opt_state), field)     return loss_and_field[0], loss_and_field[1], update_fun(k, gradient, opt_state)  pbar = tqdm(range(100)) field = -src for k in pbar:     lossval, field, opt_state = update(opt_state, field)     # For logging     pbar.set_description(\"Ampl: {:01.4f}\".format(-lossval))     losshistory.append(lossval)      transmit_phase = get_params(opt_state) <pre>Ampl: 0.1289: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [02:09&lt;00:00,  1.29s/it]\n</pre> In\u00a0[30]: Copied! <pre>optimal_field = field.on_grid\nplt.figure(figsize=(10,8))\nplt.imshow(jnp.abs(optimal_field), cmap=\"inferno\", vmax=0.1)\nplt.colorbar()\n</pre> optimal_field = field.on_grid plt.figure(figsize=(10,8)) plt.imshow(jnp.abs(optimal_field), cmap=\"inferno\", vmax=0.1) plt.colorbar() Out[30]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f7d43624fd0&gt;</pre> In\u00a0[37]: Copied! <pre>plt.figure(figsize=(10,2.5))\nplt.imshow(jax.nn.sigmoid(get_params(opt_state)).T, cmap=\"magma\")\nplt.colorbar()\n</pre> plt.figure(figsize=(10,2.5)) plt.imshow(jax.nn.sigmoid(get_params(opt_state)).T, cmap=\"magma\") plt.colorbar() Out[37]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f7d6c254730&gt;</pre> In\u00a0[38]: Copied! <pre>plt.plot(losshistory)\n</pre> plt.plot(losshistory) Out[38]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f7fc80f56d0&gt;]</pre> In\u00a0[39]: Copied! <pre>import matplotlib\n</pre> import matplotlib In\u00a0[42]: Copied! <pre>fig, ax = plt.subplots(1,2,figsize=(8,3), gridspec_kw={'width_ratios': [3, 1]})\n\nim1 = ax[0].imshow(jnp.abs(optimal_field), cmap=\"viridis\", vmax=.1)\nax[0].axis(\"off\")\nax[0].scatter(210,70, marker='x', color=\"red\", label=\"Target\", alpha=0.8)\nplt.colorbar(im1, ax=ax[0])\nax[0].set_title(\"Wavefield amplitude\")\nax[0].legend(loc=\"lower right\")\n\nsos_opt = get_sos(get_params(opt_state)).on_grid\nim2 = ax[1].imshow(sos_opt[44:212,108:148], cmap=\"binary\", vmin=1, vmax=2)\nplt.colorbar(im2, ax=ax[1])\nax[1].set_title(\"Optimized lens\")\nax[1].axis(\"off\")\n\n#plt.savefig('img/optimized_lens.pgf')\n</pre> fig, ax = plt.subplots(1,2,figsize=(8,3), gridspec_kw={'width_ratios': [3, 1]})  im1 = ax[0].imshow(jnp.abs(optimal_field), cmap=\"viridis\", vmax=.1) ax[0].axis(\"off\") ax[0].scatter(210,70, marker='x', color=\"red\", label=\"Target\", alpha=0.8) plt.colorbar(im1, ax=ax[0]) ax[0].set_title(\"Wavefield amplitude\") ax[0].legend(loc=\"lower right\")  sos_opt = get_sos(get_params(opt_state)).on_grid im2 = ax[1].imshow(sos_opt[44:212,108:148], cmap=\"binary\", vmin=1, vmax=2) plt.colorbar(im2, ax=ax[1]) ax[1].set_title(\"Optimized lens\") ax[1].axis(\"off\")  #plt.savefig('img/optimized_lens.pgf') Out[42]: <pre>(-0.5, 39.5, 167.5, -0.5)</pre>"},{"location":"notebooks/simulate_helmholtz_equation.html#optimize-acoustic-simulations","title":"Optimize acoustic simulations\u00b6","text":""},{"location":"notebooks/simulate_helmholtz_equation.html#domain","title":"Domain\u00b6","text":"<p>First, we need to define the domain where the PDE is solved. In the current version of the software, domains are always represented as scalings of the unit hyptercube centered at the origin. Furthermore, an intrinsic discretization also needs to be provided, which is only usd by grid-based methods, such as Finite Differences or Fourier spectral methods.</p> <p>Let's define a domain implicitly using a grid of size $256 \\times 256$ and $dx=1$</p>"},{"location":"notebooks/simulate_helmholtz_equation.html#a-first-operator","title":"A first operator\u00b6","text":"<p>First, note that the $\\sigma$ function has a direct dependence on the coordinate value. Therefore our first step is to define a <code>Coordinate</code> field whose value is equal to $\\mathbf{x}$:</p>"},{"location":"notebooks/simulate_helmholtz_equation.html#pml-function","title":"PML function\u00b6","text":"<p>We can now build on top of this new operator to construct more complex ones. In general, we can transform an arbitrary composition of operators described in a function using the <code>operator</code> decorator</p>"},{"location":"notebooks/simulate_helmholtz_equation.html#modified-laplacian-operator","title":"Modified Laplacian operator\u00b6","text":"<p>The next step is to define the Laplacian operatr $\\nabla^2$ using the modified derivatives</p> <p>$$ \\partial_{x_j} = \\frac{\\partial_{x_j}}{\\gamma_j} $$</p> <p>That is, we need to define a function over an input field $u$ that evaluates</p> <p>$$ \\sum_{j=\\{x,y\\}} \\frac{\\partial_{x_j}}{\\gamma_j} \\left( \\frac{\\partial_{x_j}}{\\gamma_j} u \\right) $$</p>"},{"location":"notebooks/simulate_helmholtz_equation.html#helmholtz-operator","title":"Helmholtz operator\u00b6","text":""},{"location":"notebooks/simulate_helmholtz_equation.html#optimise-the-speed-of-sound-of-a-lens","title":"Optimise the speed of sound of a lens\u00b6","text":"<p>The following code reproduces the example in the NeurIPS paper. It finds a lens to focus an acoustic source on a target, using gradient descent</p>"},{"location":"operators/differential.html","title":"<code>jaxdf.operators.differential</code>","text":""},{"location":"operators/differential.html#operators","title":"Operators","text":""},{"location":"operators/differential.html#derivative","title":"<code>derivative</code>","text":"<p>Given a field $u$, it returns the field</p> <p>$$ \\frac{\\partial}{\\partial \\epsilon} u, \\qquad \\epsilon \\in {x, y, \\dots } $$</p> Concrete implementations: <code>derivative(x: 'Continuous', *, axis = 0, params = None) </code> <p>Derivative operator for continuous fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>Continuous field</p>required<code>axis</code><p>Axis along which to take the derivative</p><code>0</code><p>Returns:</p>TypeDescription<code></code><p>Continuous field</p></p>"},{"location":"operators/differential.html#diag_jacobian","title":"<code>diag_jacobian</code>","text":"<p>Given a vector field $u = (u_x,u_y,\\dots)$ with the same dimensions as the dimensions of the domain, it returns the diagonal of the Jacobian matrix</p> <p>$$ \\left( \\frac{\\partial u_x}{\\partial x}, \\frac{\\partial u_y}{\\partial y}, \\dots \\right) $$</p> Concrete implementations: <code>diag_jacobian(x: 'Continuous', *, params = None) </code> <p>Diagonal Jacobian operator for continuous fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>Continuous field</p>required<p>Returns:</p>TypeDescription<code></code><p>The diagonal Jacobian of the field</p></p> <code>diag_jacobian(x: 'FiniteDifferences', *, stagger = [0], params = None) </code> <p>Diagonal Jacobian operator for finite differences fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>FiniteDifferences field</p>required<code>stagger</code><p>Stagger of the derivative</p><code>[0]</code><p>Returns:</p>TypeDescription<code></code><p>The diagonal Jacobian of the field</p></p> <code>diag_jacobian(x: 'FourierSeries', *, stagger = [0], correct_nyquist = True, params = None) </code> <p>Diagonal Jacobian operator for Fourier series fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><code>FourierSeries</code><p>Input field</p>required<code>stagger</code><code>list</code><p>Staggering value for the returned fields. The fields are staggered in the direction of their derivative. Defaults to [0].</p><code>[0]</code><code>correct_nyquist</code><code>bool</code><p>If <code>True</code>, uses a correction of the derivative filter for the Nyquist frequency, which preserves Hermitian symmetric and null space. See those notes for more details. Defaults to True.</p><code>True</code><p>Returns:</p>TypeDescription<code></code><p>The vector field whose components are the diagonal entries of the Jacobian of the input field.</p></p>"},{"location":"operators/differential.html#gradient","title":"<code>gradient</code>","text":"<p>Given a field $u$, it returns the vector field</p> <p>$$ \\nabla u = \\left(\\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y}, \\dots\\right) $$</p> Concrete implementations: <code>gradient(x: 'Continuous', *, params = None) </code> <p>Gradient operator for continuous fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>Continuous field</p>required<p>Returns:</p>TypeDescription<code></code><p>The gradient of the field</p></p> <code>gradient(x: 'FiniteDifferences', *, stagger = [0], params = None) </code> <p>Gradient operator for finite differences fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>FiniteDifferences field</p>required<code>stagger</code><p>Stagger of the derivative</p><code>[0]</code><p>Returns:</p>TypeDescription<code></code><p>The gradient of the field</p></p> <code>gradient(x: 'FourierSeries', *, stagger = [0], correct_nyquist = True, params = None) </code> <p>Gradient operator for Fourier series fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><code>FourierSeries</code><p>Input field</p>required<code>stagger</code><code>list</code><p>Staggering value for the returned fields. The fields are staggered in the direction of their derivative. Defaults to [0].</p><code>[0]</code><code>correct_nyquist</code><code>bool</code><p>If <code>True</code>, uses a correction of the derivative filter for the Nyquist frequency, which preserves Hermitian symmetric and null space. See those notes for more details. Defaults to True.</p><code>True</code><p>Returns:</p>TypeDescription<code>FourierSeries</code><p>The gradient of the input field.</p></p>"},{"location":"operators/differential.html#heterog_laplacian","title":"<code>heterog_laplacian</code>","text":"<p>Given a field $u$ and a cofficient field $c$, it returns the field</p> <p>$$ \\nabla_c^2 u = \\nabla \\cdot (c \\nabla u) $$</p> Concrete implementations: <code>heterog_laplacian(x: 'FourierSeries', c: 'FourierSeries', *, params = None) </code> <p>Computes the position-varying laplacian using algorithm 4 of [Johnson, 2011].<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><code>FourierSeries</code><p>Input field</p>required<code>c</code><code>FourierSeries</code><p>Coefficient field</p>required<p>Returns:</p>TypeDescription<code></code><p>The Laplacian of the field</p></p>"},{"location":"operators/differential.html#laplacian","title":"<code>laplacian</code>","text":"<p>Given a scalar field $u$, it returns the scalar field</p> <p>$$ \\nabla^2 u = \\nabla \\cdot \\nabla u = \\sum_{\\epsilon \\in {x,y,\\dots}} \\frac{\\partial^2 u}{\\partial \\epsilon^2} $$</p> Concrete implementations: <code>laplacian(x: 'Continuous', *, params = None) </code> <p>Laplacian operator for continuous fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>Continuous field</p>required<p>Returns:</p>TypeDescription<code></code><p>The Laplacian of the field</p></p> <code>laplacian(x: 'FiniteDifferences', *, params = None) </code> <p>Gradient operator for finite differences fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>FiniteDifferences field</p>required<p>Returns:</p>TypeDescription<code></code><p>The gradient of the field</p></p> <code>laplacian(x: 'FourierSeries', *, params = None) </code> <p>Laplacian operator for Fourier series fields.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><code>FourierSeries</code><p>Input field</p>required<p>Returns:</p>TypeDescription<code></code><p>The Laplacian of the field</p></p>"},{"location":"operators/differential.html#utilities","title":"Utilities","text":""},{"location":"operators/differential.html#jaxdf.operators.differential.get_fd_coefficients","title":"<code>get_fd_coefficients(x, order=1, stagger=0)</code>","text":"<p>Returnst the stencil coefficients for a 1D Finite Differences derivative operator.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>FiniteDifferences</code> <p>FiniteDifferences field</p> required <code>order</code> <code>int</code> <p>Order of the derivative</p> <code>1</code> <code>stagger</code> <code>Union[float, int]</code> <p>Stagger of the derivative</p> <code>0</code> <p>Returns:</p> Type Description <p>Stencil coefficients</p>"},{"location":"operators/differential.html#jaxdf.operators.differential.fd_derivative_init","title":"<code>fd_derivative_init(x, axis=0, stagger=0, *args, **kwargs)</code>","text":"<p>Initializes the stencils for FiniteDifferences derivatives. Accepts an arbitrary number of positional and keyword arguments after the mandatory arguments, which are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>FiniteDifferences</code> <p>FiniteDifferences field</p> required <code>axis</code> <p>Axis along which to take the derivative</p> <code>0</code> <code>stagger</code> <p>Stagger of the derivative</p> <code>0</code> <p>Returns:</p> Type Description <p>Stencil coefficients</p>"},{"location":"operators/differential.html#jaxdf.operators.differential.fd_diag_jacobian_init","title":"<code>fd_diag_jacobian_init(x, stagger, *args, **kwargs)</code>","text":"<p>Initializes the parameters for the diagonal Jacobian of a FiniteDifferences field. Accepts an arbitrary number of positional and keyword arguments after the mandatory arguments, which are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>FiniteDifferences</code> <p>FiniteDifferences field</p> required <code>stagger</code> <p>Stagger of the derivative</p> required <p>Returns:</p> Type Description <p>Stencil coefficients</p>"},{"location":"operators/functions.html","title":"<code>jaxdf.operators.functions</code>","text":""},{"location":"operators/functions.html#operators","title":"Operators","text":""},{"location":"operators/functions.html#compose","title":"<code>compose</code>","text":"<p>Implements a decorator that allows to compose <code>jax</code> functions with fields. Given a function $f$ and a <code>Field</code> $x$, the result is a new field representing</p> <p>$$ y = f(x) $$</p> <p>The usage of the decorator is as follows: <pre><code>y = compose(x)(f)\n</code></pre></p> Concrete implementations: <code>compose(x: 'Continuous', *, params = None) </code> <p>Applies function composition on the <code>get_fun</code> of the Continuous object.</p> <code>compose(x: 'OnGrid', *, params = None) </code> <p>Maps the given function over the pytree of parameters of the <code>Field</code>.</p> <code>compose(x: 'object', *, params = None) </code> <p>For non-field objects, the composition is simply the application of the <code>jax</code> function to the input.</p> <pre><code>compose(x)(fun) == fun(x)\n</code></pre>"},{"location":"operators/functions.html#functional","title":"<code>functional</code>","text":"<p>It works exactly like <code>compose</code>, but the function <code>f</code> maps one or more fields to a scalar value.</p> <p>The usage of the decorator is as follows: <pre><code>y = functional(x)(f)\n</code></pre></p> <p>It is useful to improve the readibility of the code.</p> <p>For non-field objects, a functional is simply the application of the <code>jax</code> function to the input.</p> <pre><code>functional(x)(fun) == fun(x)\n</code></pre> Concrete implementations: <code>functional(x: 'OnGrid', *, params = None) </code> <p>Maps the given function over the parameters of the field</p> <p>Example</p> <pre><code>x = OnGrid(params=-1.0, ...)\ny = functional(x)(jnp.sum)\ny.params # This is -1.0\n</code></pre> <code>functional(x: 'object', *, params = None) </code> <p>For non-field objects, a functional is simply the application of the <code>jax</code> function to the input.</p> <pre><code>functional(x)(fun) == fun(x)\n</code></pre>"},{"location":"operators/functions.html#get_component","title":"<code>get_component</code>","text":"<p>This operator $A(u, \\text{dim})$ which has the signature <code>(u: Field, dim: int) -&gt; Field</code>. It returns the component of the field $u$ at the dimension $dim$.</p> <p>$$ u(x) = (u_0(x), u_1(x), \\ldots, u_N(x)) \\to u_{\\text{dim}}(x) $$</p> Concrete implementations: <code>get_component(x: 'OnGrid', *, dim: 'int', params = None) </code> <p>Slices the parameters of the field along the last dimensions, at the index specified by <code>dim</code>.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>The field to slice</p>required<code>dim</code><p>The index to slice at</p>required<p>Returns:</p>TypeDescription<code></code><p>A new 1D field corresponding to the <code>dim</code>-th component of the input field.</p></p>"},{"location":"operators/functions.html#shift_operator","title":"<code>shift_operator</code>","text":"<p>Implements the shift operator $S(\\Delta x)$ which is used to shift (spatially) a field $u$ by a constant $\\Delta x$:</p> <p>$$ v = S(\\Delta x) u = u(x - \\Delta x) $$</p> Concrete implementations: <code>shift_operator(x: 'Continuous', *, dx: 'object', params = None) </code> <p>Shifts the field by <code>dx</code> using function composition.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>The field to shift</p>required<code>dx</code><p>The shift to apply</p>required<p>Returns:</p>TypeDescription<code></code><p>A new field corresponding to the shifted input field.</p></p> <code>shift_operator(x: 'FiniteDifferences', *, dx = [0.0], params = None) </code> <p>Shifts the field by <code>dx</code> using finite differences.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>The field to shift</p>required<code>dx</code><p>The shift to apply. It is ignored if the <code>params</code> argument is not <code>None</code>.</p><code>[0.0]</code><p>Returns:</p>TypeDescription<code></code><p>A new field corresponding to the shifted input field.</p></p> <code>shift_operator(x: 'FourierSeries', *, dx = [0], params = None) </code> <p>Shifts the field by <code>dx</code> using the shift theorem in Fourier space.<p>Parameters:</p>NameTypeDescriptionDefault<code>x</code><p>The field to shift</p>required<code>dx</code><p>The shift to apply</p><code>[0]</code><p>Returns:</p>TypeDescription<code></code><p>A new field corresponding to the shifted input field.</p></p>"},{"location":"operators/functions.html#sum_over_dims","title":"<code>sum_over_dims</code>","text":"<p>Reduces a vector field $u = (u_x, u_y, \\dots)$ to a scalar field by summing over the dimensions:</p> <p>$$ v = \\sum_{i \\in {x,y,\\dots}} u_i $$</p> Concrete implementations: <code>sum_over_dims(x: 'Continuous', *, params = None) </code> <code>sum_over_dims(x: 'OnGrid', *, params = None) </code>"},{"location":"operators/functions.html#utilities","title":"Utilities","text":""},{"location":"operators/functions.html#jaxdf.operators.functions.fd_shift_kernels","title":"<code>fd_shift_kernels(x, dx, *args, **kwargs)</code>","text":"<p>Computes the shift kernels for FiniteDifferences fields.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>FiniteDifferences</code> <p>The field to shift</p> required <code>dx</code> <code>List[float]</code> <p>The shift to apply</p> required <p>Returns:</p> Type Description <p>The kernel to apply to the field coefficients in order to</p> <p>shift the field.</p>"},{"location":"operators/linear_algebra.html","title":"<code>jaxdf.operators.linear_algebra</code>","text":""},{"location":"operators/linear_algebra.html#dot_product","title":"<code>dot_product</code>","text":"<p>Returns the dot product $u \\cdot v$ between two vector fields $u$ and $v$.</p> Concrete implementations: <code>dot_product(x: 'OnGrid', y: 'OnGrid', *, params = None) </code> <p>Computes the dot product of two fields.</p>"}]}